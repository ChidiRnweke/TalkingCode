{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Question \ud83e\uddd1\ud83c\udfff\u200d\ud83d\udcbb: What is the purpose of this project? </p> <p>TalkingCode \ud83e\udd16: As a machine learning engineer and web developer, my GitHub repositories showcase my academic and personal projects, highlighting my skills in areas such as web development, machine learning, and data science. While my very advanced projects are not open-source, the repositories available offer a glimpse into my coding capabilities and project involvement. </p>"},{"location":"#rag-what","title":"... RAG what?","text":"<p>TalkingCode is a retrieval augmented generation (RAG) chatbot that has access to all of my GitHub repositories. In case you're unfamiliar with RAG chatbots, the basic idea is that you summarize each of the code files in your repository into one or more lists of numbers, also known as embeddings. When a user sends a message to the chatbot, the message is also summarized into an embedding. These embeddings are then compared to find the most similar embeddings. The original code files are then appended to the original question as additional context. The chatbot then generates a response based on the context. The response is generated using a language model. The chatbot is able to generate responses that are relevant to the user's message.</p>"},{"location":"#the-components-of-the-project","title":"The components of the project","text":"<p>The project exists out of 3 components:</p> <ol> <li>A data pipeline that embeds the code files in my GitHub repositories.</li> <li>A backend that provides an API to the chatbot.</li> <li>A frontend that provides a user interface to the chatbot.</li> </ol> <p>The first two components are written in Python using respectively the Dagster and FastAPI frameworks. The frontend is written in TypeScript using sveltekit and TailwindCSS.</p> <p>Each of these components has its own section in the documentation. The documentation is structured in such a way that you can pick up any of the components and read that section without needing to read the other sections. You can also read the documentation from start to finish to get a full understanding of the project.</p> <p>Currently, the project uses OpenAI's <code>text-embedding-large</code> and <code>gpt-4o</code> for respectively the embeddings and the completions model. The embeddings are stored in a postgres database with the vector extension. A more detailed overview is the image below.</p> <p></p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! At this point what I need the most is feedback on the project. If you have any feedback, please open an issue. Suggestions for new features are also welcome. If you want to contribute code, please open an issue first so we can discuss the feature or bug fix you want to work on.</p>"},{"location":"backend/","title":"Overview","text":""},{"location":"backend/#introduction","title":"Introduction","text":"<p>This documentation is focused on the backend of the TalkingCode project. The backend is a FastAPI application that serves the endpoints for the frontend. The backend is responsible for handling the requests from the frontend, processing the requests, and returning the responses. The backend is also responsible for interacting with the database and the external APIs.</p>"},{"location":"backend/#structure","title":"Structure","text":"<p>The architecture offers an overview of the backend. On a high level, the backend is divided into the following components:</p> <ul> <li>Main: The main module that starts the FastAPI application.</li> <li>Config: The configuration module that loads the configuration from the environment variables.</li> <li>Errors: The errors module that defines the custom exceptions that are used in the backend.</li> <li>Services: The services module that contains the services that interact with the database and the external APIs.</li> </ul> <p>Most of the logic of the backend is in the services. The project makes use of dependency injection to facilitate testing. The services are designed to be testable. The services are tested using a combination of unit and integration tests. The tests are written using the <code>pytest</code> framework. The tests are located in the <code>tests</code> directory. You can read more about the testing strategy here.</p>"},{"location":"backend/#endpoints","title":"Endpoints","text":"<p>The backend exposes the following endpoints:</p> <ul> <li><code>/chat</code>: The endpoint that handles the chat requests. The endpoint accepts a JSON payload with the user's message and returns a JSON payload with the bot's response. The endpoint uses the <code>RetrievalAugmentedGeneration</code> service to generate the bot's response.</li> <li><code>/remaining-space</code>: The endpoint that handles the remaining space requests. The endpoint returns the remaining space in the database. The endpoint uses the <code>SQLRetrievalService</code> to retrieve the remaining space in the database.</li> </ul> <p>You can read more about them in the OpenAPI documentation here.</p>"},{"location":"backend/#reference","title":"Reference","text":"<p>You should start by reading the main module. The main module is the entry point of the FastAPI application. It contains the configuration of the FastAPI application and the routes that are exposed by the FastAPI application. From there on you can read the other modules in any order. </p>"},{"location":"backend/#extras","title":"Extras","text":"<p>Make sure to check out my personal website's dedicated page for instructions on how to host the project yourself. The instructions are detailed and should be easy to follow. If you have any questions, feel free to reach out to me.</p> <p>You should also check out the source code of the project on GitHub.</p> <p>Enjoy the documentation!</p>"},{"location":"backend/architecture/","title":"Architecture","text":"<p>The architecture of the backend isn't following any specific pattern but is rather a loose collection of what I consider to be best practices for any backend. </p>"},{"location":"backend/architecture/#api-layer","title":"API layer","text":"<p>The API layer is responsible for handling incoming requests and dealing with possible errors. It is built with FastAPI. There is no business logic in the API layer. It is solely responsible for serializing, deserializing, and routing requests to the right service.</p>"},{"location":"backend/architecture/#error-handling","title":"Error handling","text":"<p>The error handling is done using custom exceptions and the \"app error\" pattern. I encourage you to read the errors module for more information as well as the relevant handler defined in the API layer. </p> <p>The summary is that custom exceptions are defined in the <code>errors</code> module. These exceptions are easily created (ang logged) using the <code>map_errors</code> context manager. The exceptions are caught in the API layer and converted to a JSON response. A fixed set of errors is defined in the <code>errors</code> module. The errors are defined in a way that they can be easily extended.</p> <p>In the future I will integrate a solution like Sentry to log the errors.</p>"},{"location":"backend/architecture/#services","title":"Services","text":"<p>The main \"service\" is the <code>RetrievalAugmentedGeneration</code> service. This service is responsible for generating the bot's response. It carries a number of dependencies that are injected into it. The dependencies are the <code>EmbeddingService</code>,<code>GenerationService</code> and <code>RetrievalService</code>. The <code>EmbeddingService</code> is responsible for converting the user's message into an embedding. The <code>GenerationService</code> is responsible for generating a response from an embedding. The <code>RetrievalService</code> is responsible for retrieving the most similar embeddings from the database.</p> <p>Using dependency injection makes the service testable. The dependencies can be easily stubbed out. This service is tested using a combination of unit and integration tests. The tests are written using the <code>pytest</code> framework. You can read more about the testing strategy here.</p>"},{"location":"backend/architecture/#database","title":"Database","text":"<p>The database used is PostgresSQL with the <code>pgVector</code> extension. This turns the database into a two-in-one database. It can store regular data as well as vectors. The vectors are used to store the embeddings of the user's messages. The database is accessed using the <code>SQLRetrievalService</code>. The <code>SQLRetrievalService</code> is responsible for retrieving the most similar embeddings from the database. The database is spun up using <code>Testcontainers</code> in the integration tests.</p>"},{"location":"backend/architecture/#configuration","title":"Configuration","text":"<p>The configuration is loaded from the environment variables. The configuration is loaded in the <code>config</code> module. The configuration is loaded in the <code>main</code> module and passed to the FastAPI application. The configuration is used to configure the services among other things. A full list of the configuration options can be found here.</p>"},{"location":"backend/endpoints/","title":"Endpoints","text":"<p>The openapi documentation is embedded below. It is only properly visible in light mode. If you are using dark mode, you can switch to light mode by clicking the button in the top right corner of the page.</p> <p></p>"},{"location":"backend/testing/","title":"Testing","text":"<p>The testing strategy of the backend is using a combination of unit and integration tests. Both are written using the <code>pytest</code> framework. You can find the tests in the <code>tests</code> directory. Naturally, the unit tests are faster than the integration tests. The integration tests are slower because they require spinning up a PostgreSQL database using <code>Testcontainers</code>. You can run them separately using the <code>pytest /unit</code> or <code>pytest /integration</code> command.</p>"},{"location":"backend/testing/#unit-tests","title":"Unit tests","text":"<p>As much as possible, the backend is designed to be testable. The main units that are tested are the services. The services are the main components that interact with the database and the external APIs. The services are tested by writing stubs for the dependencies that they have. The tests are written in a way that they test the real behavior of the services.</p> <p>These tests are low effort to write and maintain. They are also fast to run. They don't require spinning up a database or making API calls. The issue is that they don't test the interaction between the different parts of the backend. This is where the integration tests come in. </p>"},{"location":"backend/testing/#integration-tests","title":"Integration tests","text":"<p>Integration tests actually capture a lot of the complexity of the backend. They test the interaction between the different parts of the backend. The tests are written in a way that they test the real behavior of the backend. The tests are run using <code>pytest</code>. The tests are run in the same event loop. This has pros and cons. The pros are that the tests are faster and that the database is only created once. The cons are that the tests are not isolated from each other. The practical implication is that you more or less need to know what the other tests are doing otherwise tests may pass or fail unexpectedly. I'm the only one working on this project, so I'm fine with this.</p>"},{"location":"backend/testing/#setup","title":"Setup","text":"<p>The general strategy for integration tests is to test the interaction between different parts of the backend. FastAPI is only used to expose the endpoints, so the tests are not run through the API. Instead, the tests are run directly on the backend code. </p> <p>The tests are run using <code>pytest</code>. The test files are located in the <code>tests/integration</code> directory. The tests are run using the <code>pytest</code> command. </p> <p><code>Testcontainers</code> is used to spin up a PostgreSQL database for the tests. The database is created and destroyed for each test run. The database is seeded with test data using the <code>database_fixtures</code> fixture.</p>"},{"location":"backend/testing/#test-structure","title":"Test structure","text":"<p>The main units that have to be tested are the <code>SQLRetrievalService</code>'s public methods as well as the <code>RetrievalAugmentedGeneration</code> service. The former is tested by using the fixtures that are provided and testing its real behavior. The latter is trickier because it depends on <code>EmbeddingService</code> and <code>GenerationService</code>. The concrete implementations require calling the real API and incurring costs, which is not desirable in a test. The solution around this is making stubs for these services and testing the behavior of the <code>RetrievalAugmentedGeneration</code> service.</p> <p>You're encouraged to read the tests in the <code>tests/integration</code> directory to get a better understanding of how the tests work. I won't go into detail here because it's not the focus of this documentation. </p>"},{"location":"backend/reference/config/","title":"Config","text":""},{"location":"backend/reference/config/#backend.config.AppConfig","title":"<code>AppConfig</code>  <code>dataclass</code>","text":"<p>This class is used to store the configuration for the application. It is obtained from the environment variables and passed to the application components that need it.</p> <p>You can instantiate it directly or use the <code>from_env</code> method to create an instance from the environment variables. The latter is the recommended way to create an instance. When certain environment variables are not set, it will log a warning if the application can still run without them, or raise an exception if the application cannot run without them.</p> <p>Attributes:</p> Name Type Description <code>embedding_model</code> <code>str</code> <p>The name of the text embedding model to use. Can be set with the <code>EMBEDDING_MODEL</code> environment variable.</p> <code>top_k</code> <code>int</code> <p>The number of top candidates to return from the model. Can be set with the <code>TOP_K</code> environment variable.</p> <code>chat_model</code> <code>str</code> <p>The name of the chat model to use. Can be set with the <code>CHAT_MODEL</code> environment variable.</p> <code>async_session</code> <code>async_sessionmaker[AsyncSession]</code> <p>The async session maker for the database. Can be set with the <code>ASYNC_DATABASE_URL</code> environment variable.</p> <code>system_prompt</code> <code>str</code> <p>The system prompt to use for the chat model. Can be set with the <code>SYSTEM_PROMPT</code> environment variable.</p> <code>openAI_client</code> <code>AsyncOpenAI</code> <p>The OpenAI client to use for making requests. Can be set with the <code>OPENAI_EMBEDDING_API_KEY</code> environment variable.</p> <code>max_spend</code> <code>float</code> <p>The maximum amount of money that can be spent in a day. Can be set with the <code>MAX_SPEND</code> environment variable.</p> Source code in <code>backend/backend/config.py</code> <pre><code>@dataclass(frozen=True)\nclass AppConfig:\n    \"\"\"\n    This class is used to store the configuration for the application. It is obtained\n    from the environment variables and passed to the application components that need\n    it.\n\n    You can instantiate it directly or use the `from_env` method to create an instance\n    from the environment variables. The latter is the recommended way to create an\n    instance. When certain environment variables are not set, it will log a warning if the\n    application can still run without them, or raise an exception if the application\n    cannot run without them.\n\n    Attributes:\n        embedding_model (str): The name of the text embedding model to use.\n            Can be set with the `EMBEDDING_MODEL` environment variable.\n        top_k (int): The number of top candidates to return from the model.\n            Can be set with the `TOP_K` environment variable.\n        chat_model (str): The name of the chat model to use.\n            Can be set with the `CHAT_MODEL` environment variable.\n        async_session (async_sessionmaker[AsyncSession]): The async session maker for the\n            database.\n            Can be set with the `ASYNC_DATABASE_URL` environment variable.\n        system_prompt (str): The system prompt to use for the chat model.\n            Can be set with the `SYSTEM_PROMPT` environment variable.\n        openAI_client (AsyncOpenAI): The OpenAI client to use for making requests.\n            Can be set with the `OPENAI_EMBEDDING_API_KEY` environment variable.\n        max_spend (float): The maximum amount of money that can be spent in a day.\n            Can be set with the `MAX_SPEND` environment variable.\n    \"\"\"\n\n    embedding_model: str\n    top_k: int\n    chat_model: str\n    async_session: async_sessionmaker[AsyncSession]\n    system_prompt: str\n    openAI_client: AsyncOpenAI\n    max_spend: float\n\n    @classmethod\n    def from_env(cls, log: Logger | None = None) -&gt; Self:\n        \"\"\"Create an instance of AppConfig from the environment variables.\n\n        Args:\n            log (Logger | None, optional): If provided, it logs whether or not the\n                environment variables were found. Defaults to None.\n\n        Returns:\n           (AppConfig): The configuration object with the values from the environment\n        \"\"\"\n        Session = configure_async_session_maker(log)\n        embedding_model = env_var_or_default(\n            \"EMBEDDING_MODEL\", \"text-embedding-3-large\", log\n        )\n        chat_model = env_var_or_default(\"CHAT_MODEL\", \"gpt-4o\", log)\n        open_ai_key = env_var_or_throw(\"OPENAI_EMBEDDING_API_KEY\", log)\n        top_k = int(env_var_or_default(\"TOP_K\", \"5\", log))\n        max_spend = float(env_var_or_default(\"MAX_SPEND\", \"1.5\", log))\n        openAI_client = AsyncOpenAI(api_key=open_ai_key)\n        system_prompt = env_var_or_default(\n            \"SYSTEM_PROMPT\",\n            \"You are a helpful assistant.\",\n            log,\n        )\n\n        config = cls(\n            embedding_model=embedding_model,\n            chat_model=chat_model,\n            async_session=Session,\n            openAI_client=openAI_client,\n            top_k=top_k,\n            system_prompt=system_prompt,\n            max_spend=max_spend,\n        )\n        return config\n</code></pre>"},{"location":"backend/reference/config/#backend.config.AppConfig.from_env","title":"<code>from_env(log=None)</code>  <code>classmethod</code>","text":"<p>Create an instance of AppConfig from the environment variables.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>Logger | None</code> <p>If provided, it logs whether or not the environment variables were found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>AppConfig</code> <p>The configuration object with the values from the environment</p> Source code in <code>backend/backend/config.py</code> <pre><code>@classmethod\ndef from_env(cls, log: Logger | None = None) -&gt; Self:\n    \"\"\"Create an instance of AppConfig from the environment variables.\n\n    Args:\n        log (Logger | None, optional): If provided, it logs whether or not the\n            environment variables were found. Defaults to None.\n\n    Returns:\n       (AppConfig): The configuration object with the values from the environment\n    \"\"\"\n    Session = configure_async_session_maker(log)\n    embedding_model = env_var_or_default(\n        \"EMBEDDING_MODEL\", \"text-embedding-3-large\", log\n    )\n    chat_model = env_var_or_default(\"CHAT_MODEL\", \"gpt-4o\", log)\n    open_ai_key = env_var_or_throw(\"OPENAI_EMBEDDING_API_KEY\", log)\n    top_k = int(env_var_or_default(\"TOP_K\", \"5\", log))\n    max_spend = float(env_var_or_default(\"MAX_SPEND\", \"1.5\", log))\n    openAI_client = AsyncOpenAI(api_key=open_ai_key)\n    system_prompt = env_var_or_default(\n        \"SYSTEM_PROMPT\",\n        \"You are a helpful assistant.\",\n        log,\n    )\n\n    config = cls(\n        embedding_model=embedding_model,\n        chat_model=chat_model,\n        async_session=Session,\n        openAI_client=openAI_client,\n        top_k=top_k,\n        system_prompt=system_prompt,\n        max_spend=max_spend,\n    )\n    return config\n</code></pre>"},{"location":"backend/reference/config/#backend.config.configure_async_session_maker","title":"<code>configure_async_session_maker(log=None)</code>","text":"<p>Create an async session maker for the database. It uses the <code>ASYNC_DATABASE_URL</code> environment variable to connect to the database.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>Logger | None</code> <p>If provided, it logs whether or not the connection string was found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>async_sessionmaker[AsyncSession]</code> <p>async_sessionmaker[AsyncSession]: The async session maker for the database.</p> Source code in <code>backend/backend/config.py</code> <pre><code>def configure_async_session_maker(\n    log: Logger | None = None,\n) -&gt; async_sessionmaker[AsyncSession]:\n    \"\"\"Create an async session maker for the database.\n    It uses the `ASYNC_DATABASE_URL` environment variable to connect to the database.\n\n\n    Args:\n        log (Logger | None, optional): If provided, it logs whether or not the\n            connection string was found. Defaults to None.\n\n    Returns:\n        async_sessionmaker[AsyncSession]: The async session maker for the database.\n    \"\"\"\n    conn_string = env_var_or_default(\n        \"ASYNC_DATABASE_URL\",\n        \"postgresql+asyncpg://postgres:postgres@localhost/chatGITpt\",\n        log,\n    )\n    engine = create_async_engine(conn_string)\n    return async_sessionmaker(engine, expire_on_commit=False)\n</code></pre>"},{"location":"backend/reference/errors/","title":"Errors","text":""},{"location":"backend/reference/errors/#backend.errors.AppError","title":"<code>AppError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for exceptions in this module. The strategy for handling errors is similar to the error enum pattern in Rust and Scala. The idea is to have a single error type that can be used to represent all the different types of errors that can occur in the application.</p> <p>For example, if an error occurs in the infrastructure layer, we can raise an InfraError. If the maximum spend for the day has been reached, we can raise a MaximumSpendError. If the input provided by the user is invalid, we can raise an InputError.</p> <p>This allows us to handle a single error type in the application layer, which makes it easier to reason about the error handling logic and to provide a consistent error handling experience to the user.</p> <p>FastAPI provides a way to handle exceptions globally using the exception handler middleware. We can use this middleware to catch the AppError and return an appropriate response to the user.</p> <p>In practice this pattern doesn't come to life in Python as it does in Rust or Scala because there is no real sum type, it is mimicked with inheritance but we can't force exhaustive pattern matching to handle all cases this way.</p> Source code in <code>backend/backend/errors.py</code> <pre><code>class AppError(Exception):\n    \"\"\"Base class for exceptions in this module.\n    The strategy for handling errors is similar to the error enum pattern in Rust\n    and Scala. The idea is to have a single error type that can be used to represent\n    all the different types of errors that can occur in the application.\n\n    For example, if an error occurs in the infrastructure layer, we can raise an\n    InfraError. If the maximum spend for the day has been reached, we can raise a\n    MaximumSpendError. If the input provided by the user is invalid, we can raise an\n    InputError.\n\n    This allows us to handle a single error type in the application layer, which makes\n    it easier to reason about the error handling logic and to provide a consistent\n    error handling experience to the user.\n\n    FastAPI provides a way to handle exceptions globally using the exception handler\n    middleware. We can use this middleware to catch the AppError and return an\n    appropriate response to the user.\n\n    In practice this pattern doesn't come to life in Python as it does in Rust or Scala\n    because there is no real sum type, it is mimicked with inheritance but we can't force\n    exhaustive pattern matching to handle all cases this way.\n    \"\"\"\n\n    def __init__(self, err: Exception | None) -&gt; None:\n        self.original_error = err\n</code></pre>"},{"location":"backend/reference/errors/#backend.errors.InfraError","title":"<code>InfraError</code>","text":"<p>               Bases: <code>AppError</code></p> <p>This error is raised when an error occurs in the infrastructure layer. In practice, these are errors that are out of the control of the application code, such as network errors, database errors, etc.</p> Source code in <code>backend/backend/errors.py</code> <pre><code>class InfraError(AppError):\n    \"\"\"\n    This error is raised when an error occurs in the infrastructure layer. In practice,\n    these are errors that are out of the control of the application code, such as\n    network errors, database errors, etc.\n    \"\"\"\n\n    def __init__(self, err: Exception) -&gt; None:\n        super().__init__(err)\n        self.msg = \"An error occurred in the infrastructure layer\"\n\n    def __str__(self) -&gt; str:\n        return self.msg\n</code></pre>"},{"location":"backend/reference/errors/#backend.errors.InputError","title":"<code>InputError</code>","text":"<p>               Bases: <code>AppError</code></p> <p>Raised when the input provided by the user is invalid. These are cases that are not caught by the validation logic in the API layer (Pydantic models).</p> Source code in <code>backend/backend/errors.py</code> <pre><code>class InputError(AppError):\n    \"\"\"Raised when the input provided by the user is invalid. These are cases\n    that are not caught by the validation logic in the API layer (Pydantic models).\n    \"\"\"\n\n    def __init__(\n        self, message: str | None = None, err: Exception | None = None\n    ) -&gt; None:\n        if message:\n            self.msg = message\n        else:\n            self.msg = \"The input you provided is invalid\"\n        super().__init__(err)\n\n    def __str__(self) -&gt; str:\n        return self.msg\n</code></pre>"},{"location":"backend/reference/errors/#backend.errors.MaximumSpendError","title":"<code>MaximumSpendError</code>","text":"<p>               Bases: <code>AppError</code></p> <p>Raised when the maximum spend for the day has been reached. The maximum spend is configurable through the <code>MAX_SPEND</code> environment variable.</p> Source code in <code>backend/backend/errors.py</code> <pre><code>class MaximumSpendError(AppError):\n    \"\"\"Raised when the maximum spend for the day has been reached.\n    The maximum spend is configurable through the `MAX_SPEND` environment variable.\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(None)\n        self.msg = \"The maximum spend for the day has been reached\"\n\n    def __str__(self) -&gt; str:\n        return self.msg\n</code></pre>"},{"location":"backend/reference/errors/#backend.errors.map_errors","title":"<code>map_errors(map_to=InfraError)</code>","text":"<p>A context manager that catches exceptions, logs them and raises them as AppError. Ideally, this context manager should be used in the infrastructure layer to catch exceptions and raise them as an AppError. This allows us to handle all errors in a consistent way and provide a better error handling experience to the user.</p> <p>This should have been a decorator but I gave up on it because of the type hinting complexity.</p> <p>Parameters:</p> Name Type Description Default <code>map_to</code> <code>Type[AppError]</code> <p>The type you want to map it to. Defaults to InfraError.</p> <code>InfraError</code> <p>Raises:</p> Type Description <code>AppError</code> <p>The exception mapped to the type specified in the <code>map_to</code> parameter.</p> Source code in <code>backend/backend/errors.py</code> <pre><code>@contextmanager\ndef map_errors(map_to: Type[AppError] = InfraError):\n    \"\"\"A context manager that catches exceptions, logs them and raises them as AppError.\n    Ideally, this context manager should be used in the infrastructure layer to catch\n    exceptions and raise them as an AppError. This allows us to handle all errors in\n    a consistent way and provide a better error handling experience to the user.\n\n    This should have been a decorator but I gave up on it because of the type hinting\n    complexity.\n\n    Args:\n        map_to (Type[AppError], optional): The type you want to map it to.\n            Defaults to InfraError.\n\n    Raises:\n        (AppError): The exception mapped to the type specified in the `map_to` parameter.\n    \"\"\"\n    try:\n        yield\n    except Exception as e:\n        log.error(f\"Error in: {e}\", exc_info=True)\n        raise map_to(err=e) from e\n</code></pre>"},{"location":"backend/reference/foo/","title":"Foo","text":""},{"location":"backend/reference/main/","title":"Main","text":""},{"location":"backend/reference/main/#backend.main.chat","title":"<code>chat(question, session=Depends(get_session))</code>  <code>async</code>","text":"<p>This function is used to handle the chat endpoint. It is used to handle the incoming chat requests and generate the response using the RAG model. The RAG model is used to retrieve the context, embed the text, and generate the response. The response is then returned to the user.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>InputQuery</code> <p>The input query object.</p> required <code>session</code> <code>AsyncSession</code> <p>The async session object. This is provided by the FastAPI dependency injection.</p> <code>Depends(get_session)</code> <p>Returns:</p> Type Description <code>RAGResponse</code> <p>The response object containing the response and the session ID.</p> Source code in <code>backend/backend/main.py</code> <pre><code>@app.post(\"/\")\nasync def chat(\n    question: InputQuery, session: AsyncSession = Depends(get_session)\n) -&gt; RAGResponse:\n    \"\"\"\n    This function is used to handle the chat endpoint. It is used to handle the incoming\n    chat requests and generate the response using the RAG model. The RAG model is used to\n    retrieve the context, embed the text, and generate the response. The response is then\n    returned to the user.\n\n    Args:\n        question (InputQuery): The input query object.\n        session (AsyncSession): The async session object. This is provided by the FastAPI\n            dependency injection.\n\n    Returns:\n        (RAGResponse): The response object containing the response and the session ID.\n    \"\"\"\n    max_spend = get_max_spend()\n    rag = RetrievalAugmentedGeneration(\n        embedding_service=get_openai_embedding_service(),\n        generation_service=get_openai_generation_service(),\n        retrieval_service=SQLRetrievalService(session),\n        max_spend=max_spend,\n        date=date.today(),\n    )\n    return await rag.retrieval_augmented_generation(question, get_top_k())\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_chat_model","title":"<code>get_chat_model()</code>","text":"<p>This function is used to get the chat model from the application configuration. It is required by the <code>GenerationService</code> to generate the response.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the chat model.</p> Source code in <code>backend/backend/main.py</code> <pre><code>def get_chat_model() -&gt; str:\n    \"\"\"\n    This function is used to get the chat model from the application configuration.\n    It is required by the `GenerationService` to generate the response.\n\n    Returns:\n        (str): The name of the chat model.\n    \"\"\"\n    return app_config[\"config\"].chat_model\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_embedding_model","title":"<code>get_embedding_model()</code>","text":"<p>This function is used to get the embedding model from the application configuration. It is required by the <code>EmbeddingService</code> to embed the text.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the text embedding model.</p> Source code in <code>backend/backend/main.py</code> <pre><code>def get_embedding_model() -&gt; str:\n    \"\"\"\n    This function is used to get the embedding model from the application configuration.\n    It is required by the `EmbeddingService` to embed the text.\n\n    Returns:\n        (str): The name of the text embedding model.\n    \"\"\"\n    return app_config[\"config\"].embedding_model\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_max_spend","title":"<code>get_max_spend()</code>","text":"<p>This function is used to get the maximum spend from the application configuration. It is used to determine the maximum amount of money that can be spent in a day.</p> <p>Returns:</p> Type Description <code>float</code> <p>The maximum amount of money that can be spent in a day.</p> Source code in <code>backend/backend/main.py</code> <pre><code>def get_max_spend() -&gt; float:\n    \"\"\"\n    This function is used to get the maximum spend from the application configuration.\n    It is used to determine the maximum amount of money that can be spent in a day.\n\n    Returns:\n        (float): The maximum amount of money that can be spent in a day.\n    \"\"\"\n    return app_config[\"config\"].max_spend\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_openAI_client","title":"<code>get_openAI_client()</code>","text":"<p>This function is used to get the OpenAI client from the application configuration. It returns the OpenAI client from the application configuration.</p> <p>Returns:</p> Type Description <code>AsyncOpenAI</code> <p>The OpenAI client.</p> Source code in <code>backend/backend/main.py</code> <pre><code>def get_openAI_client() -&gt; AsyncOpenAI:\n    \"\"\"\n    This function is used to get the OpenAI client from the application configuration.\n    It returns the OpenAI client from the application configuration.\n\n    Returns:\n        (AsyncOpenAI): The OpenAI client.\n    \"\"\"\n    return app_config[\"config\"].openAI_client\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_openai_embedding_service","title":"<code>get_openai_embedding_service()</code>","text":"<p>This function is used to get the OpenAI embedding service. It is used to embed the text using the OpenAI API. All of the required dependencies are transitively provided by the application configuration singleton.</p> <p>Returns:</p> Type Description <code>OpenAIEmbeddingService</code> <p>The OpenAI embedding service.</p> Source code in <code>backend/backend/main.py</code> <pre><code>def get_openai_embedding_service() -&gt; OpenAIEmbeddingService:\n    \"\"\"\n    This function is used to get the OpenAI embedding service.\n    It is used to embed the text using the OpenAI API. All of the required dependencies\n    are transitively provided by the application configuration singleton.\n\n    Returns:\n        (OpenAIEmbeddingService): The OpenAI embedding service.\n    \"\"\"\n    openAI_client = get_openAI_client()\n    embedding_model = get_embedding_model()\n    return OpenAIEmbeddingService(client=openAI_client, embedding_model=embedding_model)\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_openai_generation_service","title":"<code>get_openai_generation_service()</code>","text":"<p>This function is used to get the OpenAI generation service. It is used to generate the response using the OpenAI API. All of the required dependencies are transitively provided by the application configuration singleton.</p> <p>Returns:</p> Type Description <code>OpenAIGenerationService</code> <p>The OpenAI generation service.</p> Source code in <code>backend/backend/main.py</code> <pre><code>def get_openai_generation_service() -&gt; OpenAIGenerationService:\n    \"\"\"\n    This function is used to get the OpenAI generation service.\n    It is used to generate the response using the OpenAI API. All of the required dependencies\n    are transitively provided by the application configuration singleton.\n\n    Returns:\n        (OpenAIGenerationService): The OpenAI generation service.\n    \"\"\"\n    openAI_client = get_openAI_client()\n    chat_model = get_chat_model()\n    system_prompt = get_system_prompt()\n\n    return OpenAIGenerationService(\n        client=openAI_client,\n        chat_model=chat_model,\n        system_prompt=system_prompt,\n    )\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_session","title":"<code>get_session()</code>  <code>async</code>","text":"<p>This function is used to get an async session from the application configuration. It draws the session from the application configuration and yields it to the caller. This way we are certain that a new session is created for each request.</p> <p>Returns:</p> Type Description <code>AsyncGenerator[AsyncSession, None]</code> <p>AsyncGenerator[AsyncSession, None]: The async session generator.</p> <p>Yields:</p> Type Description <code>Iterator[AsyncGenerator[AsyncSession, None]]</code> <p>The async session generator.</p> Source code in <code>backend/backend/main.py</code> <pre><code>async def get_session() -&gt; AsyncGenerator[AsyncSession, None]:\n    \"\"\"\n    This function is used to get an async session from the application configuration.\n    It draws the session from the application configuration and yields it to the caller.\n    This way we are certain that a new session is created for each request.\n\n    Returns:\n        AsyncGenerator[AsyncSession, None]: The async session generator.\n\n    Yields:\n        (Iterator[AsyncGenerator[AsyncSession, None]]): The async session generator.\n    \"\"\"\n    async with app_config[\"config\"].async_session() as session:\n        yield session\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_system_prompt","title":"<code>get_system_prompt()</code>","text":"<p>This function is used to get the system prompt from the application configuration. It is required by the <code>GenerationService</code> to generate the response.</p> <p>Returns:</p> Type Description <code>str</code> <p>The system prompt to use for the chat model.</p> Source code in <code>backend/backend/main.py</code> <pre><code>def get_system_prompt() -&gt; str:\n    \"\"\"\n    This function is used to get the system prompt from the application configuration.\n    It is required by the `GenerationService` to generate the response.\n\n    Returns:\n        (str): The system prompt to use for the chat model.\n    \"\"\"\n    return app_config[\"config\"].system_prompt\n</code></pre>"},{"location":"backend/reference/main/#backend.main.get_top_k","title":"<code>get_top_k()</code>","text":"<p>This function is used to get the top_k value from the application configuration. It is used to determine the number of top candidates to return from the model.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of top candidates to return from the model.</p> Source code in <code>backend/backend/main.py</code> <pre><code>def get_top_k() -&gt; int:\n    \"\"\"\n    This function is used to get the top_k value from the application configuration.\n    It is used to determine the number of top candidates to return from the model.\n\n    Returns:\n        (int): The number of top candidates to return from the model.\n    \"\"\"\n    return app_config[\"config\"].top_k\n</code></pre>"},{"location":"backend/reference/main/#backend.main.handle_app_errors","title":"<code>handle_app_errors(request, exc)</code>  <code>async</code>","text":"<p>This function is used to handle the application errors globally. It uses the app error pattern discussed in <code>reference/errors</code> in the documentation. All the application errors are caught and converted to an <code>AppError</code>. This method handles the specific instances of the <code>AppError</code> and returns the appropriate JSON response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object.</p> required <code>exc</code> <code>AppError</code> <p>The application error that was raised.</p> required <p>Returns:</p> Type Description <code>JSONResponse</code> <p>The JSON response with the error message and status code.</p> Source code in <code>backend/backend/main.py</code> <pre><code>@app.exception_handler(AppError)\nasync def handle_app_errors(request: Request, exc: AppError) -&gt; JSONResponse:\n    \"\"\"\n    This function is used to handle the application errors globally. It uses the app error\n    pattern discussed in `reference/errors` in the documentation. All the application errors\n    are caught and converted to an `AppError`. This method handles the specific instances of\n    the `AppError` and returns the appropriate JSON response.\n\n    Args:\n        request (Request): The request object.\n        exc (AppError): The application error that was raised.\n\n    Returns:\n        (JSONResponse): The JSON response with the error message and status code.\n    \"\"\"\n    match exc:\n        case InputError(message=message):\n            return JSONResponse(message, status_code=400)\n        case InfraError():\n            return JSONResponse(str(exc), status_code=500)\n        case MaximumSpendError():\n            return JSONResponse(str(exc), status_code=402)\n        case _:\n            log.error(f\"An unhandled app error occurred: {exc}\")\n            return JSONResponse(str(exc), status_code=500)\n</code></pre>"},{"location":"backend/reference/main/#backend.main.lifespan","title":"<code>lifespan(app)</code>  <code>async</code>","text":"<p>This function is used to manage the lifespan of the FastAPI application. It is used to set up the application configuration and store it as a singleton. This singleton is then used to provide configuration to the application's dependencies. The configuration contains the database session, OpenAI client, and other configuration values.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>FastAPI</code> <p>The FastAPI application instance.</p> required Source code in <code>backend/backend/main.py</code> <pre><code>@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    This function is used to manage the lifespan of the FastAPI application.\n    It is used to set up the application configuration and store it as a singleton.\n    This singleton is then used to provide configuration to the application's dependencies.\n    The configuration contains the database session, OpenAI client, and other configuration values.\n\n    Args:\n        app (FastAPI): The FastAPI application instance.\n    \"\"\"\n    app_config[\"config\"] = AppConfig.from_env(log)\n    yield\n</code></pre>"},{"location":"backend/reference/main/#backend.main.remaining_spend","title":"<code>remaining_spend(session=Depends(get_session))</code>  <code>async</code>","text":"<p>This function is used to get the remaining spend for the day. It is used to get the remaining spend for the day by querying the database and calculating the remaining spend based on the maximum spend for the day.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>The async session object. This is provided by the FastAPI dependency injection.</p> <code>Depends(get_session)</code> <p>Returns:</p> Type Description <code>RemainingSpend</code> <p>The remaining spend object containing the remaining spend for the day.</p> Source code in <code>backend/backend/main.py</code> <pre><code>@app.get(\"/remaining_spend\")\nasync def remaining_spend(\n    session: AsyncSession = Depends(get_session),\n) -&gt; RemainingSpend:\n    \"\"\"\n    This function is used to get the remaining spend for the day. It is used to get the\n    remaining spend for the day by querying the database and calculating the remaining\n    spend based on the maximum spend for the day.\n\n    Args:\n        session (AsyncSession): The async session object. This is provided by the FastAPI\n            dependency injection.\n\n    Returns:\n        (RemainingSpend): The remaining spend object containing the remaining spend for\n            the day.\n    \"\"\"\n    max_spend = get_max_spend()\n    rag = RetrievalAugmentedGeneration(\n        embedding_service=get_openai_embedding_service(),\n        generation_service=get_openai_generation_service(),\n        retrieval_service=SQLRetrievalService(session),\n        max_spend=max_spend,\n        date=date.today(),\n    )\n    return await rag.remaining_spend()\n</code></pre>"},{"location":"backend/reference/retrieve/","title":"Retrieve","text":""},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.EmbeddedResponse","title":"<code>EmbeddedResponse</code>  <code>dataclass</code>","text":"<p>A class that represents the embedded response. This class is used to represent the response generated by the embedding service. The embedded response contains the embedding and the number of tokens spent on the embedding. This data class is used to pass the embedded response between the <code>EmbeddingService</code> and the <code>RetrievalService</code>.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>list[float]</code> <p>The embedded query.</p> required <code>token_count</code> <code>int</code> <p>The number of tokens spent on the embedding.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass EmbeddedResponse:\n    \"\"\"\n    A class that represents the embedded response. This class is used to represent the response\n    generated by the embedding service. The embedded response contains the embedding and the number\n    of tokens spent on the embedding. This data class is used to pass the embedded response between\n    the `EmbeddingService` and the `RetrievalService`.\n\n    Args:\n        embedding (list[float]): The embedded query.\n        token_count (int): The number of tokens spent on the embedding.\n    \"\"\"\n\n    embedding: list[float]\n    token_count: int\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.EmbeddingService","title":"<code>EmbeddingService</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>The interface for the embedding service. Responsible for embedding the input query. The embedded query is then used to retrieve the top k contexts based on the embedded query by the <code>RetrievalService</code>.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>class EmbeddingService(Protocol):\n    \"\"\"\n    The interface for the embedding service. Responsible for embedding the input query.\n    The embedded query is then used to retrieve the top k contexts based on the\n    embedded query by the `RetrievalService`.\n    \"\"\"\n\n    async def embed(self, text: str) -&gt; \"EmbeddedResponse\":\n        \"\"\"\n        Embeds the input text. Turns the input text into a list of floats that represent the text.\n        The embedded response also contains the number of tokens spent on the embedding.\n\n        Args:\n            text (str): The text to embed.\n\n        Returns:\n            (EmbeddedResponse): The embedded response. Contains the embedding and the number of tokens spent.\n        \"\"\"\n        ...\n\n    def get_embed_model_name(self) -&gt; str:\n        \"\"\"\n        Returns the name of the embedding model used for embedding. Needed for tracking the token spend\n        per model. Each model has a different token cost, e.g., the embedding model is typically cheaper\n        than the generation model.\n\n        Returns:\n            (str): The name of the embedding model used for embedding.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.EmbeddingService.embed","title":"<code>embed(text)</code>  <code>async</code>","text":"<p>Embeds the input text. Turns the input text into a list of floats that represent the text. The embedded response also contains the number of tokens spent on the embedding.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to embed.</p> required <p>Returns:</p> Type Description <code>EmbeddedResponse</code> <p>The embedded response. Contains the embedding and the number of tokens spent.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def embed(self, text: str) -&gt; \"EmbeddedResponse\":\n    \"\"\"\n    Embeds the input text. Turns the input text into a list of floats that represent the text.\n    The embedded response also contains the number of tokens spent on the embedding.\n\n    Args:\n        text (str): The text to embed.\n\n    Returns:\n        (EmbeddedResponse): The embedded response. Contains the embedding and the number of tokens spent.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.EmbeddingService.get_embed_model_name","title":"<code>get_embed_model_name()</code>","text":"<p>Returns the name of the embedding model used for embedding. Needed for tracking the token spend per model. Each model has a different token cost, e.g., the embedding model is typically cheaper than the generation model.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the embedding model used for embedding.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>def get_embed_model_name(self) -&gt; str:\n    \"\"\"\n    Returns the name of the embedding model used for embedding. Needed for tracking the token spend\n    per model. Each model has a different token cost, e.g., the embedding model is typically cheaper\n    than the generation model.\n\n    Returns:\n        (str): The name of the embedding model used for embedding.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.GenerationService","title":"<code>GenerationService</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>The interface for the generation service. Responsible for generating the response given the input query and the retrieved contexts.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>class GenerationService(Protocol):\n    \"\"\"\n    The interface for the generation service. Responsible for generating the response\n    given the input query and the retrieved contexts.\n    \"\"\"\n\n    async def augmented_generation(\n        self, query: \"InputQuery\", context: list[\"RetrievedContext\"]\n    ) -&gt; tuple[str, int]:\n        \"\"\"Answers the user's query with the retrieved context.\n        Requires that you already have the context retrieved from the `RetrievalService`.\n\n        Args:\n            query (InputQuery): The input query. Contains the user's question, and optionally,\n                the previous context and session ID.\n            context (list[RetrievedContext]): The list of retrieved contexts. Each one contains\n                the distance, file name, repository name, path in the repository, extension, and URL.\n\n        Returns:\n            (tuple[str, int]): The response generated by the model and the number of tokens spent.\n        \"\"\"\n        ...\n\n    def get_chat_model_name(self) -&gt; str:\n        \"\"\"Returns the name of the chat model used for generation. Needed for tracking the token spend\n           per model. Each model has a different token cost, e.g., the embedding model is typically cheaper\n           than the generation model.\n\n\n        Returns:\n            (str): The name of the chat model used for generation.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.GenerationService.augmented_generation","title":"<code>augmented_generation(query, context)</code>  <code>async</code>","text":"<p>Answers the user's query with the retrieved context. Requires that you already have the context retrieved from the <code>RetrievalService</code>.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>InputQuery</code> <p>The input query. Contains the user's question, and optionally, the previous context and session ID.</p> required <code>context</code> <code>list[RetrievedContext]</code> <p>The list of retrieved contexts. Each one contains the distance, file name, repository name, path in the repository, extension, and URL.</p> required <p>Returns:</p> Type Description <code>tuple[str, int]</code> <p>The response generated by the model and the number of tokens spent.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def augmented_generation(\n    self, query: \"InputQuery\", context: list[\"RetrievedContext\"]\n) -&gt; tuple[str, int]:\n    \"\"\"Answers the user's query with the retrieved context.\n    Requires that you already have the context retrieved from the `RetrievalService`.\n\n    Args:\n        query (InputQuery): The input query. Contains the user's question, and optionally,\n            the previous context and session ID.\n        context (list[RetrievedContext]): The list of retrieved contexts. Each one contains\n            the distance, file name, repository name, path in the repository, extension, and URL.\n\n    Returns:\n        (tuple[str, int]): The response generated by the model and the number of tokens spent.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.GenerationService.get_chat_model_name","title":"<code>get_chat_model_name()</code>","text":"<p>Returns the name of the chat model used for generation. Needed for tracking the token spend    per model. Each model has a different token cost, e.g., the embedding model is typically cheaper    than the generation model.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the chat model used for generation.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>def get_chat_model_name(self) -&gt; str:\n    \"\"\"Returns the name of the chat model used for generation. Needed for tracking the token spend\n       per model. Each model has a different token cost, e.g., the embedding model is typically cheaper\n       than the generation model.\n\n\n    Returns:\n        (str): The name of the chat model used for generation.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.InputQuery","title":"<code>InputQuery</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic model for the input query. This class is client facing and is used to validate the input query. The first question in the conversation will not have any previous context and will not have a session ID. The subsequent questions will have a session ID and previous context. The previous context is a list of question-answer pairs from the previous interactions. The session ID is used to track the token spend for the given session.</p> <p>If the session ID is provided, the previous context must also be provided otherwise a <code>ValueError</code> is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the session ID is provided but the previous context is not provided or vice versa.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>class InputQuery(BaseModel):\n    \"\"\"\n    A Pydantic model for the input query. This class is client facing and is used to validate the input query.\n    The first question in the conversation will not have any previous context and will not have a session ID.\n    The subsequent questions will have a session ID and previous context. The previous context is a list of\n    question-answer pairs from the previous interactions. The session ID is used to track the token spend for\n    the given session.\n\n    If the session ID is provided, the previous context must also be provided otherwise a `ValueError` is raised.\n\n    Raises:\n        (ValueError): If the session ID is provided but the previous context is not provided or vice versa.\n    \"\"\"\n\n    query: str\n    previous_context: Optional[list[PreviousQAs]] = None\n    session_id: Optional[str] = None\n\n    def previous_context_to_dict(self) -&gt; list[dict[str, str]]:\n        \"\"\"\n        Transforms the previous context from a list of `PreviousQAs` to a list of dictionaries.\n        This is the exact format that OpenAI's SDK expects for its completion API.\n\n        Returns:\n            list[dict[str, str]]: The previous context as a list of dictionaries.\n        \"\"\"\n        res = []\n        if self.previous_context:\n            for qa in self.previous_context:\n                res.append({\"role\": \"user\", \"content\": qa.question})\n                res.append({\"role\": \"assistant\", \"content\": qa.answer})\n        return res\n\n    @model_validator(mode=\"after\")\n    def _validate_previous_context(self) -&gt; Self:\n        \"\"\"\n        Validates the input query. Ensures that the session ID is provided if the previous context is provided\n        and vice versa.\n\n        Raises:\n            (ValueError): If the session ID is provided but the previous context is not provided or vice versa.\n\n        \"\"\"\n        if self.session_id and not self.previous_context:\n            raise ValueError(\n                \"Previous context must be provided if session_id is present.\"\n            )\n        elif not self.session_id and self.previous_context:\n            raise ValueError(\n                \"Session ID must be provided if previous context is present.\"\n            )\n        return self\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.InputQuery.previous_context_to_dict","title":"<code>previous_context_to_dict()</code>","text":"<p>Transforms the previous context from a list of <code>PreviousQAs</code> to a list of dictionaries. This is the exact format that OpenAI's SDK expects for its completion API.</p> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict[str, str]]: The previous context as a list of dictionaries.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>def previous_context_to_dict(self) -&gt; list[dict[str, str]]:\n    \"\"\"\n    Transforms the previous context from a list of `PreviousQAs` to a list of dictionaries.\n    This is the exact format that OpenAI's SDK expects for its completion API.\n\n    Returns:\n        list[dict[str, str]]: The previous context as a list of dictionaries.\n    \"\"\"\n    res = []\n    if self.previous_context:\n        for qa in self.previous_context:\n            res.append({\"role\": \"user\", \"content\": qa.question})\n            res.append({\"role\": \"assistant\", \"content\": qa.answer})\n    return res\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.OpenAIEmbeddingService","title":"<code>OpenAIEmbeddingService</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EmbeddingService</code></p> <p>A class that performs text embedding using OpenAI's API. The class is responsible for embedding the input text. The embedded text is then used to retrieve the top k contexts based on the embedded query by the <code>RetrievalService</code>. The class is a wrapper around OpenAI's SDK and provides a more testable and maintainable interface. It implements the <code>EmbeddingService</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>The OpenAI client used for text embedding. Relies on the <code>OPENAI_EMBEDDING_API_KEY</code> environment variable.</p> required <code>embedding_model</code> <code>str</code> <p>The name of the embedding model used for text embedding. In practice this is <code>text-embedding-3-large</code> but it is configurable through environment variables. It can directly be set with the <code>EMBEDDING_MODEL</code>.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@dataclass(frozen=True)\nclass OpenAIEmbeddingService(EmbeddingService):\n    \"\"\"\n    A class that performs text embedding using OpenAI's API. The class is responsible for embedding\n    the input text. The embedded text is then used to retrieve the top k contexts based on the embedded\n    query by the `RetrievalService`. The class is a wrapper around OpenAI's SDK and provides a more\n    testable and maintainable interface. It implements the `EmbeddingService` interface.\n\n    Args:\n        client (AsyncOpenAI): The OpenAI client used for text embedding.\n            Relies on the `OPENAI_EMBEDDING_API_KEY` environment variable.\n        embedding_model (str): The name of the embedding model used for text embedding.\n            In practice this is `text-embedding-3-large` but it is configurable through environment variables.\n            It can directly be set with the `EMBEDDING_MODEL`.\n    \"\"\"\n\n    client: AsyncOpenAI\n    embedding_model: str\n\n    async def embed(self, text: str) -&gt; EmbeddedResponse:\n        \"\"\"\n        Embeds the input text. Turns the input text into a list of floats that represent the text.\n        The embedded response also contains the number of tokens spent on the embedding.\n\n        Args:\n            text (str): The text to embed.\n\n        Returns:\n            EmbeddedResponse: The embedded response. Contains the embedding and the number of tokens spent.\n        \"\"\"\n        with map_errors():\n            response = await self.client.embeddings.create(\n                input=[text], model=self.embedding_model\n            )\n        return EmbeddedResponse(\n            embedding=response.data[0].embedding,\n            token_count=response.usage.total_tokens,\n        )\n\n    def get_embed_model_name(self) -&gt; str:\n        \"\"\"\n        Wrapper around the `embedding_model` attribute. Necessary to conform to the `EmbeddingService` interface.\n\n        Returns:\n            str: The name of the embedding model used for embedding.\n        \"\"\"\n        return self.embedding_model\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.OpenAIEmbeddingService.embed","title":"<code>embed(text)</code>  <code>async</code>","text":"<p>Embeds the input text. Turns the input text into a list of floats that represent the text. The embedded response also contains the number of tokens spent on the embedding.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to embed.</p> required <p>Returns:</p> Name Type Description <code>EmbeddedResponse</code> <code>EmbeddedResponse</code> <p>The embedded response. Contains the embedding and the number of tokens spent.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def embed(self, text: str) -&gt; EmbeddedResponse:\n    \"\"\"\n    Embeds the input text. Turns the input text into a list of floats that represent the text.\n    The embedded response also contains the number of tokens spent on the embedding.\n\n    Args:\n        text (str): The text to embed.\n\n    Returns:\n        EmbeddedResponse: The embedded response. Contains the embedding and the number of tokens spent.\n    \"\"\"\n    with map_errors():\n        response = await self.client.embeddings.create(\n            input=[text], model=self.embedding_model\n        )\n    return EmbeddedResponse(\n        embedding=response.data[0].embedding,\n        token_count=response.usage.total_tokens,\n    )\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.OpenAIEmbeddingService.get_embed_model_name","title":"<code>get_embed_model_name()</code>","text":"<p>Wrapper around the <code>embedding_model</code> attribute. Necessary to conform to the <code>EmbeddingService</code> interface.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the embedding model used for embedding.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>def get_embed_model_name(self) -&gt; str:\n    \"\"\"\n    Wrapper around the `embedding_model` attribute. Necessary to conform to the `EmbeddingService` interface.\n\n    Returns:\n        str: The name of the embedding model used for embedding.\n    \"\"\"\n    return self.embedding_model\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.OpenAIGenerationService","title":"<code>OpenAIGenerationService</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GenerationService</code></p> <p>A class that performs text generation using OpenAI's API. The class is responsible for generating the response given the input query and the retrieved contexts. The class is a wrapper around OpenAI's SDK and provides a more testable and maintainable interface. It implements the <code>GenerationService</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>The OpenAI client used for text generation. Relies on the <code>OPENAI_EMBEDDING_API_KEY</code> environment variable.</p> required <code>chat_model</code> <code>str</code> <p>The name of the chat model used for text generation. Can be set with the <code>CHAT_MODEL</code> environment variable.</p> required <code>system_prompt</code> <code>str</code> <p>The system prompt used for text generation. Can be set with the <code>SYSTEM_PROMPT</code> environment variable.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass OpenAIGenerationService(GenerationService):\n    \"\"\"\n    A class that performs text generation using OpenAI's API. The class is responsible for generating\n    the response given the input query and the retrieved contexts. The class is a wrapper around OpenAI's\n    SDK and provides a more testable and maintainable interface. It implements the `GenerationService` interface.\n\n    Args:\n        client (AsyncOpenAI): The OpenAI client used for text generation.\n            Relies on the `OPENAI_EMBEDDING_API_KEY` environment variable.\n        chat_model (str): The name of the chat model used for text generation.\n            Can be set with the `CHAT_MODEL` environment variable.\n        system_prompt (str): The system prompt used for text generation.\n            Can be set with the `SYSTEM_PROMPT` environment variable.\n    \"\"\"\n\n    client: AsyncOpenAI\n    chat_model: str\n    system_prompt: str\n\n    async def augmented_generation(\n        self, query: InputQuery, context: list[RetrievedContext]\n    ) -&gt; tuple[str, int]:\n        \"\"\"\n        Answers the user's query with the retrieved context. Requires that you already have the context\n        retrieved from the `RetrievalService`.\n\n        Args:\n            query (InputQuery): The input query. Contains the user's question, and optionally,\n                the previous context and session ID.\n            context (list[RetrievedContext]): The list of retrieved contexts. Each one contains\n                the distance, file name, repository name, path in the repository, extension, and URL.\n\n        Returns:\n            (tuple[str, int]): The response generated by the model and the number of tokens spent.\n        \"\"\"\n        ctx = await asyncio.gather(*[c.to_context() for c in context])\n        model_input = self._create_model_input(query, ctx)\n        with map_errors():\n            response = await self.client.chat.completions.create(\n                model=self.chat_model,\n                messages=model_input,  # type: ignore\n            )\n        result = response.choices[0].message.content or \"\"\n        tokens = response.usage.total_tokens if response.usage else 0\n        return add_sources(result, context), tokens\n\n    def get_chat_model_name(self) -&gt; str:\n        \"\"\"Returns the name of the chat model used for generation. Needed for tracking the token spend\n            per model. Each model has a different token cost, e.g., the embedding model is typically cheaper\n            than the generation model.\n\n        Returns:\n            (str): The name of the chat model used for generation.\n        \"\"\"\n        return self.chat_model\n\n    def _create_model_input(\n        self, input: InputQuery, ctx: list[str]\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Creates the model input for the generation API. The model input is a list of dictionaries.\"\"\"\n        query_with_ctx = f\"\"\"The user's question is {input.query}.\n        You have access to additional context in a list of code files: {ctx}\"\"\"\n\n        previous_qas = input.previous_context_to_dict()\n\n        model_input = [{\"role\": \"system\", \"content\": self.system_prompt}]\n        model_input.extend(previous_qas)\n        model_input.append({\"role\": \"user\", \"content\": query_with_ctx})\n        return model_input\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.OpenAIGenerationService.augmented_generation","title":"<code>augmented_generation(query, context)</code>  <code>async</code>","text":"<p>Answers the user's query with the retrieved context. Requires that you already have the context retrieved from the <code>RetrievalService</code>.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>InputQuery</code> <p>The input query. Contains the user's question, and optionally, the previous context and session ID.</p> required <code>context</code> <code>list[RetrievedContext]</code> <p>The list of retrieved contexts. Each one contains the distance, file name, repository name, path in the repository, extension, and URL.</p> required <p>Returns:</p> Type Description <code>tuple[str, int]</code> <p>The response generated by the model and the number of tokens spent.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def augmented_generation(\n    self, query: InputQuery, context: list[RetrievedContext]\n) -&gt; tuple[str, int]:\n    \"\"\"\n    Answers the user's query with the retrieved context. Requires that you already have the context\n    retrieved from the `RetrievalService`.\n\n    Args:\n        query (InputQuery): The input query. Contains the user's question, and optionally,\n            the previous context and session ID.\n        context (list[RetrievedContext]): The list of retrieved contexts. Each one contains\n            the distance, file name, repository name, path in the repository, extension, and URL.\n\n    Returns:\n        (tuple[str, int]): The response generated by the model and the number of tokens spent.\n    \"\"\"\n    ctx = await asyncio.gather(*[c.to_context() for c in context])\n    model_input = self._create_model_input(query, ctx)\n    with map_errors():\n        response = await self.client.chat.completions.create(\n            model=self.chat_model,\n            messages=model_input,  # type: ignore\n        )\n    result = response.choices[0].message.content or \"\"\n    tokens = response.usage.total_tokens if response.usage else 0\n    return add_sources(result, context), tokens\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.OpenAIGenerationService.get_chat_model_name","title":"<code>get_chat_model_name()</code>","text":"<p>Returns the name of the chat model used for generation. Needed for tracking the token spend     per model. Each model has a different token cost, e.g., the embedding model is typically cheaper     than the generation model.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the chat model used for generation.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>def get_chat_model_name(self) -&gt; str:\n    \"\"\"Returns the name of the chat model used for generation. Needed for tracking the token spend\n        per model. Each model has a different token cost, e.g., the embedding model is typically cheaper\n        than the generation model.\n\n    Returns:\n        (str): The name of the chat model used for generation.\n    \"\"\"\n    return self.chat_model\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.PreviousQAs","title":"<code>PreviousQAs</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic model for the previous question-answer pairs. This is used to provide the model with additional context from previous interactions. It is a pydantic model because it is client facing and is used to validate the input query. This model is used in the <code>InputQuery</code> model.</p> Fields <p>question (str): A previously asked question asked by the user. answer (str): A previously received answer provided by the assistant.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>class PreviousQAs(BaseModel):\n    \"\"\"\n    A Pydantic model for the previous question-answer pairs. This is used to provide the model with\n    additional context from previous interactions. It is a pydantic model because it is client\n    facing and is used to validate the input query. This model is used in the `InputQuery` model.\n\n    Fields:\n        question (str): A previously asked question asked by the user.\n        answer (str): A previously received answer provided by the assistant.\n    \"\"\"\n\n    question: str\n    answer: str\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RAGResponse","title":"<code>RAGResponse</code>  <code>dataclass</code>","text":"<p>A class that represents the response generated by the model and the session ID. This data class is client facing and will be deserialized to JSON and returned to the user. It contains a session_id because the client needs to provide the session ID in the subsequent questions for tracking the token spend for the given session.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass RAGResponse:\n    \"\"\"\n    A class that represents the response generated by the model and the session ID.\n    This data class is client facing and will be deserialized to JSON and returned to the user.\n    It contains a session_id because the client needs to provide the session ID in the subsequent\n    questions for tracking the token spend for the given session.\n    \"\"\"\n\n    response: str\n    session_id: str\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RemainingSpend","title":"<code>RemainingSpend</code>  <code>dataclass</code>","text":"<p>Class that represents the remaining spend based on the current spend and the maximum spend limit. This is the object that will later be deserialized to JSON and returned to the user.</p> <p>Parameters:</p> Name Type Description Default <code>remaining_spend</code> <code>float</code> <p>The remaining spend based on the current spend and the maximum spend limit.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass RemainingSpend:\n    \"\"\"\n    Class that represents the remaining spend based on the current spend and the maximum spend limit.\n    This is the object that will later be deserialized to JSON and returned to the user.\n\n    Args:\n        remaining_spend (float): The remaining spend based on the current spend and the maximum spend limit.\n    \"\"\"\n\n    remaining_spend: float\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievalAugmentedGeneration","title":"<code>RetrievalAugmentedGeneration</code>  <code>dataclass</code>","text":"<p>Class that performs retrieval-augmented generation given an input query and k value.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_service</code> <code>EmbeddingService</code> <p>The embedding service used for text embedding.</p> required <code>retrieval_service</code> <code>RetrievalService</code> <p>The retrieval service used for context retrieval.</p> required <code>generation_service</code> <code>GenerationService</code> <p>The generation service used for text generation.</p> required <code>max_spend</code> <code>float</code> <p>The maximum spend limit for the retrieval-augmented generation.</p> required <code>date</code> <code>date</code> <p>The date of the retrieval-augmented generation.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@dataclass(frozen=True)\nclass RetrievalAugmentedGeneration:\n    \"\"\"\n    Class that performs retrieval-augmented generation given an input query and k value.\n\n    Args:\n        embedding_service: The embedding service used for text embedding.\n        retrieval_service: The retrieval service used for context retrieval.\n        generation_service: The generation service used for text generation.\n        max_spend: The maximum spend limit for the retrieval-augmented generation.\n        date (datetime.date): The date of the retrieval-augmented generation.\n    \"\"\"\n\n    embedding_service: \"EmbeddingService\"\n    retrieval_service: \"RetrievalService\"\n    generation_service: \"GenerationService\"\n    max_spend: float\n    date: date\n\n    async def retrieval_augmented_generation(\n        self, input: \"InputQuery\", k: int\n    ) -&gt; \"RAGResponse\":\n        \"\"\"\n        Performs retrieval-augmented generation given an input query and k value.\n        The method enforces the spend limit, validates the session ID, retrieves the top k contexts,\n\n        Args:\n            input (InputQuery): The input query. Contains the user's question, and optionally,\n                the previous context and session ID.\n            k (int): The number of contexts to retrieve.\n\n        Returns:\n            (RAGResponse): The response generated by the model and the session ID.\n\n        Raises:\n            (MaximumSpendError): If the current spend is greater than or equal to the maximum spend limit.\n        \"\"\"\n        await self._enforce_spend_limit()\n        session_id = await self._validate_and_assign_session_id(input)\n\n        retrieved, tokens_spent = await self._retrieve_top_k(input, k)\n        store_task = self.retrieval_service.store_token_spent(\n            session_id,\n            tokens_spent,\n            self.embedding_service.get_embed_model_name(),\n        )\n        generation_task = self.generation_service.augmented_generation(input, retrieved)\n        _, response_and_tokens = await asyncio.gather(store_task, generation_task)\n        response, tokens = response_and_tokens\n        await self.retrieval_service.store_token_spent(\n            session_id, tokens, self.generation_service.get_chat_model_name()\n        )\n\n        return RAGResponse(response=response, session_id=session_id)\n\n    async def remaining_spend(self) -&gt; \"RemainingSpend\":\n        \"\"\"\n\n        Calculates the remaining spend based on the current spend and the maximum spend limit.\n        The spend is capped at 0.\n\n        Returns:\n            (RemainingSpend): The remaining spend.\n        \"\"\"\n        current_spend = await self.retrieval_service.get_current_spend(self.date)\n        remaining = round(self.max_spend - current_spend, 2)\n        remaining = max(remaining, 0)\n        return RemainingSpend(remaining)\n\n    async def _enforce_spend_limit(self) -&gt; None:\n        \"\"\"\n\n        Enforces the spend limit by checking the current spend against the maximum spend limit.\n        If the current spend is greater than or equal to the maximum spend limit, raises a `MaximumSpendError`.\n\n        Raises:\n            (MaximumSpendError): If the current spend is greater than or equal to the maximum spend limit.\n\n        \"\"\"\n        current_spend = await self.retrieval_service.get_current_spend(self.date)\n        if current_spend &gt;= self.max_spend:\n            raise MaximumSpendError()\n\n    async def _validate_and_assign_session_id(self, input: \"InputQuery\") -&gt; str:\n        \"\"\"\n\n        Validates the session ID if provided, otherwise generates a new session ID.\n        This method ensures that the session ID is valid and exists in the database.\n        This is necessary to track the token spend for the given session.\n\n        Args:\n            input (InputQuery): The input query. Contains the user's question, and optionally,\n                the previous context and session ID.\n\n        Returns:\n            (str): The session ID.\n        \"\"\"\n        if input.session_id:\n            await self.retrieval_service.validate_session_id(input.session_id)\n            session_id = input.session_id\n        else:\n            session_id = str(uuid.uuid4())\n        return session_id\n\n    async def _retrieve_top_k(\n        self, input: \"InputQuery\", k: int\n    ) -&gt; tuple[list[\"RetrievedContext\"], int]:\n        \"\"\"\n            embeds the input query and retrieves the top k contexts based on the embedded query.\n\n\n        Returns:\n            (tuple[list[RetrievedContext], int]): A tuple containing the list of retrieved contexts\n                and the number of tokens spent. see `RetrievedContext` for more information.\n        \"\"\"\n        result = await self.embedding_service.embed(input.query)\n        tokens_spent = result.token_count\n        return (await self.retrieval_service.retrieve_top_k(result, k), tokens_spent)\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievalAugmentedGeneration.remaining_spend","title":"<code>remaining_spend()</code>  <code>async</code>","text":"<p>Calculates the remaining spend based on the current spend and the maximum spend limit. The spend is capped at 0.</p> <p>Returns:</p> Type Description <code>RemainingSpend</code> <p>The remaining spend.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def remaining_spend(self) -&gt; \"RemainingSpend\":\n    \"\"\"\n\n    Calculates the remaining spend based on the current spend and the maximum spend limit.\n    The spend is capped at 0.\n\n    Returns:\n        (RemainingSpend): The remaining spend.\n    \"\"\"\n    current_spend = await self.retrieval_service.get_current_spend(self.date)\n    remaining = round(self.max_spend - current_spend, 2)\n    remaining = max(remaining, 0)\n    return RemainingSpend(remaining)\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievalAugmentedGeneration.retrieval_augmented_generation","title":"<code>retrieval_augmented_generation(input, k)</code>  <code>async</code>","text":"<p>Performs retrieval-augmented generation given an input query and k value. The method enforces the spend limit, validates the session ID, retrieves the top k contexts,</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InputQuery</code> <p>The input query. Contains the user's question, and optionally, the previous context and session ID.</p> required <code>k</code> <code>int</code> <p>The number of contexts to retrieve.</p> required <p>Returns:</p> Type Description <code>RAGResponse</code> <p>The response generated by the model and the session ID.</p> <p>Raises:</p> Type Description <code>MaximumSpendError</code> <p>If the current spend is greater than or equal to the maximum spend limit.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def retrieval_augmented_generation(\n    self, input: \"InputQuery\", k: int\n) -&gt; \"RAGResponse\":\n    \"\"\"\n    Performs retrieval-augmented generation given an input query and k value.\n    The method enforces the spend limit, validates the session ID, retrieves the top k contexts,\n\n    Args:\n        input (InputQuery): The input query. Contains the user's question, and optionally,\n            the previous context and session ID.\n        k (int): The number of contexts to retrieve.\n\n    Returns:\n        (RAGResponse): The response generated by the model and the session ID.\n\n    Raises:\n        (MaximumSpendError): If the current spend is greater than or equal to the maximum spend limit.\n    \"\"\"\n    await self._enforce_spend_limit()\n    session_id = await self._validate_and_assign_session_id(input)\n\n    retrieved, tokens_spent = await self._retrieve_top_k(input, k)\n    store_task = self.retrieval_service.store_token_spent(\n        session_id,\n        tokens_spent,\n        self.embedding_service.get_embed_model_name(),\n    )\n    generation_task = self.generation_service.augmented_generation(input, retrieved)\n    _, response_and_tokens = await asyncio.gather(store_task, generation_task)\n    response, tokens = response_and_tokens\n    await self.retrieval_service.store_token_spent(\n        session_id, tokens, self.generation_service.get_chat_model_name()\n    )\n\n    return RAGResponse(response=response, session_id=session_id)\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievalService","title":"<code>RetrievalService</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for the retrieval service. Responsible for retrieving the top k contexts based on the embedded query. The query is embedded already by the <code>EmbeddingService</code>.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>class RetrievalService(Protocol):\n    \"\"\"\n    Interface for the retrieval service. Responsible for retrieving the top k\n    contexts based on the embedded query. The query is embedded already by the\n    `EmbeddingService`.\n\n    \"\"\"\n\n    async def retrieve_top_k(\n        self, embedded_query: \"EmbeddedResponse\", k: int\n    ) -&gt; list[\"RetrievedContext\"]:\n        \"\"\"\n\n        Retrieves the top k contexts based on the embedded query.\n        The contexts are the k most relevant documents to the embedded query.\n\n        Args:\n            embedded_query (EmbeddedResponse): The embedded query.\n            k (int): The number of contexts to retrieve.\n\n        Returns:\n            (list[RetrievedContext]): The list of retrieved contexts.\n        \"\"\"\n        ...\n\n    async def store_token_spent(\n        self, session_id: str, token_count: int, model_name: str\n    ):\n        \"\"\"\n\n        Stores the token count spent for a given session ID and model name.\n        This is necessary to track the token spend for the given session. This\n        is used further down the line to calculate the current spend and enforce\n        the spend limit.\n\n        Args:\n            session_id (str): The session ID.\n            token_count (int): The number of tokens spent.\n            model_name (str): The name of the model used for token count. Different\n                models have different token costs. Embedding models are\n                typically cheaper than generation models.\n        \"\"\"\n\n        ...\n\n    async def validate_session_id(self, session_id: str) -&gt; None:\n        \"\"\"\n\n        Validates the session ID if provided. This method ensures that the session ID is valid\n        and exists in the database. This is necessary to track the token spend for the given session.\n\n        Args:\n            session_id (str): The session ID.\n        \"\"\"\n        ...\n\n    async def get_current_spend(self, date: date) -&gt; float:\n        \"\"\"\n\n        Retrieves the current spend for a given date. The date is\n        mostly given as a parameter to facilitate testing, it keeps\n        this method more pure than it is if it had to instantiate the date.\n\n        Args:\n            date (date): The date for which to retrieve the current spend.\n\n\n        Returns:\n            (float): The current spend for the given date.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievalService.get_current_spend","title":"<code>get_current_spend(date)</code>  <code>async</code>","text":"<p>Retrieves the current spend for a given date. The date is mostly given as a parameter to facilitate testing, it keeps this method more pure than it is if it had to instantiate the date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>The date for which to retrieve the current spend.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The current spend for the given date.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def get_current_spend(self, date: date) -&gt; float:\n    \"\"\"\n\n    Retrieves the current spend for a given date. The date is\n    mostly given as a parameter to facilitate testing, it keeps\n    this method more pure than it is if it had to instantiate the date.\n\n    Args:\n        date (date): The date for which to retrieve the current spend.\n\n\n    Returns:\n        (float): The current spend for the given date.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievalService.retrieve_top_k","title":"<code>retrieve_top_k(embedded_query, k)</code>  <code>async</code>","text":"<p>Retrieves the top k contexts based on the embedded query. The contexts are the k most relevant documents to the embedded query.</p> <p>Parameters:</p> Name Type Description Default <code>embedded_query</code> <code>EmbeddedResponse</code> <p>The embedded query.</p> required <code>k</code> <code>int</code> <p>The number of contexts to retrieve.</p> required <p>Returns:</p> Type Description <code>list[RetrievedContext]</code> <p>The list of retrieved contexts.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def retrieve_top_k(\n    self, embedded_query: \"EmbeddedResponse\", k: int\n) -&gt; list[\"RetrievedContext\"]:\n    \"\"\"\n\n    Retrieves the top k contexts based on the embedded query.\n    The contexts are the k most relevant documents to the embedded query.\n\n    Args:\n        embedded_query (EmbeddedResponse): The embedded query.\n        k (int): The number of contexts to retrieve.\n\n    Returns:\n        (list[RetrievedContext]): The list of retrieved contexts.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievalService.store_token_spent","title":"<code>store_token_spent(session_id, token_count, model_name)</code>  <code>async</code>","text":"<p>Stores the token count spent for a given session ID and model name. This is necessary to track the token spend for the given session. This is used further down the line to calculate the current spend and enforce the spend limit.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The session ID.</p> required <code>token_count</code> <code>int</code> <p>The number of tokens spent.</p> required <code>model_name</code> <code>str</code> <p>The name of the model used for token count. Different models have different token costs. Embedding models are typically cheaper than generation models.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def store_token_spent(\n    self, session_id: str, token_count: int, model_name: str\n):\n    \"\"\"\n\n    Stores the token count spent for a given session ID and model name.\n    This is necessary to track the token spend for the given session. This\n    is used further down the line to calculate the current spend and enforce\n    the spend limit.\n\n    Args:\n        session_id (str): The session ID.\n        token_count (int): The number of tokens spent.\n        model_name (str): The name of the model used for token count. Different\n            models have different token costs. Embedding models are\n            typically cheaper than generation models.\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievalService.validate_session_id","title":"<code>validate_session_id(session_id)</code>  <code>async</code>","text":"<p>Validates the session ID if provided. This method ensures that the session ID is valid and exists in the database. This is necessary to track the token spend for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The session ID.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def validate_session_id(self, session_id: str) -&gt; None:\n    \"\"\"\n\n    Validates the session ID if provided. This method ensures that the session ID is valid\n    and exists in the database. This is necessary to track the token spend for the given session.\n\n    Args:\n        session_id (str): The session ID.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievedContext","title":"<code>RetrievedContext</code>  <code>dataclass</code>","text":"<p>This is a domain class and is used to represent the context retrieved from the database. Its main purpose is to provide a structured representation of the context that is retrieved from the database. This simplifies testing, all of the logic relies on domain objects rather than database specific types.</p> <p>The class also provides helpers to enrich the file content with additional information such as the file name, repository name, path in the repository, and file extension. This is useful for generating the response to the user.</p> <p>Parameters:</p> Name Type Description Default <code>distance</code> <code>float</code> <p>The distance between the embedded query and the retrieved context.</p> required <code>file_name</code> <code>str</code> <p>The name of the file.</p> required <code>repository_name</code> <code>str</code> <p>The name of the repository.</p> required <code>path_in_repo</code> <code>str</code> <p>The path of the file in the repository.</p> required <code>extension</code> <code>str</code> <p>The file extension.</p> required <code>url</code> <code>str</code> <p>The URL to the file content.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass RetrievedContext:\n    \"\"\"\n    This is a domain class and is used to represent the context retrieved from the database. Its\n    main purpose is to provide a structured representation of the context that is retrieved from the\n    database. This simplifies testing, all of the logic relies on domain objects rather than database\n    specific types.\n\n    The class also provides helpers to enrich the file content with additional information such as\n    the file name, repository name, path in the repository, and file extension. This is useful for\n    generating the response to the user.\n\n    Args:\n        distance (float): The distance between the embedded query and the retrieved context.\n        file_name (str): The name of the file.\n        repository_name (str): The name of the repository.\n        path_in_repo (str): The path of the file in the repository.\n        extension (str): The file extension.\n        url (str): The URL to the file content.\n    \"\"\"\n\n    distance: float\n    file_name: str\n    repository_name: str\n    path_in_repo: str\n    extension: str\n    url: str\n\n    @classmethod\n    def from_document(\n        cls, score: float, document: GithubFileModel\n    ) -&gt; \"RetrievedContext\":\n        \"\"\"Creates a `RetrievedContext` object from the document retrieved from the database.\n\n\n        Args:\n            score (float): The distance between the embedded query and the retrieved context.\n            document (GithubFileModel): The document retrieved from the database.\n\n        Returns:\n            (RetrievedContext): The retrieved context object.\n        \"\"\"\n        return cls(\n            distance=score,\n            file_name=document.name,\n            repository_name=document.repository_name,\n            path_in_repo=document.path_in_repo,\n            extension=document.file_extension,\n            url=document.content_url,\n        )\n\n    async def to_context(self) -&gt; str:\n        \"\"\"Retrieves the file content from the URL and enriches it with additional information such as\n        the file name, repository name, path in the repository, and file extension. This may\n        improve generation quality by providing additional context to the model. The method is asynchronous\n        because it fetches the file content from the URL. The data isn't persisted as such, it is fetched\n        on-demand when needed.\n\n        Returns:\n            (str): The file content enriched with additional information.\n        \"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.get(self.url) as response:\n                with map_errors():\n                    file_content = await response.text()\n        return self._enrich_file_content(file_content)\n\n    def _enrich_file_content(self, file_content: str) -&gt; str:\n        \"\"\"Enriches the file content with additional information such as the file name, repository name,\n        path in the repository, and file extension. This may improve generation quality by providing\n        additional context to the model.\n\n        Args:\n            file_content (str): The file content.\n\n        Returns:\n            (str): The file content enriched with additional information.\n        \"\"\"\n        file_name = f\"\\nThe file name is {self.file_name}.\\n\"\n        file_place_in_project = f\"The file is located at {self.path_in_repo}.\\n\"\n        file_extension = f\"The file extension is {self.extension}.\\n\"\n        return file_content + file_name + file_place_in_project + file_extension\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievedContext.from_document","title":"<code>from_document(score, document)</code>  <code>classmethod</code>","text":"<p>Creates a <code>RetrievedContext</code> object from the document retrieved from the database.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>float</code> <p>The distance between the embedded query and the retrieved context.</p> required <code>document</code> <code>GithubFileModel</code> <p>The document retrieved from the database.</p> required <p>Returns:</p> Type Description <code>RetrievedContext</code> <p>The retrieved context object.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@classmethod\ndef from_document(\n    cls, score: float, document: GithubFileModel\n) -&gt; \"RetrievedContext\":\n    \"\"\"Creates a `RetrievedContext` object from the document retrieved from the database.\n\n\n    Args:\n        score (float): The distance between the embedded query and the retrieved context.\n        document (GithubFileModel): The document retrieved from the database.\n\n    Returns:\n        (RetrievedContext): The retrieved context object.\n    \"\"\"\n    return cls(\n        distance=score,\n        file_name=document.name,\n        repository_name=document.repository_name,\n        path_in_repo=document.path_in_repo,\n        extension=document.file_extension,\n        url=document.content_url,\n    )\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.RetrievedContext.to_context","title":"<code>to_context()</code>  <code>async</code>","text":"<p>Retrieves the file content from the URL and enriches it with additional information such as the file name, repository name, path in the repository, and file extension. This may improve generation quality by providing additional context to the model. The method is asynchronous because it fetches the file content from the URL. The data isn't persisted as such, it is fetched on-demand when needed.</p> <p>Returns:</p> Type Description <code>str</code> <p>The file content enriched with additional information.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def to_context(self) -&gt; str:\n    \"\"\"Retrieves the file content from the URL and enriches it with additional information such as\n    the file name, repository name, path in the repository, and file extension. This may\n    improve generation quality by providing additional context to the model. The method is asynchronous\n    because it fetches the file content from the URL. The data isn't persisted as such, it is fetched\n    on-demand when needed.\n\n    Returns:\n        (str): The file content enriched with additional information.\n    \"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(self.url) as response:\n            with map_errors():\n                file_content = await response.text()\n    return self._enrich_file_content(file_content)\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.SQLRetrievalService","title":"<code>SQLRetrievalService</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RetrievalService</code></p> <p>A class that performs context retrieval using SQL. The class is responsible for retrieving the top k contexts based on the embedded query. The query is embedded already by the <code>EmbeddingService</code>. The class is a wrapper around SQLAlchemy and provides a more testable and maintainable interface. It implements the <code>RetrievalService</code> interface.</p> <p>Aside from this it is also responsible for storing the token spend and validating the session ID.</p> <p>Raises:</p> Type Description <code>InputError</code> <p>If a Session ID is provided, it must already exist.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass SQLRetrievalService(RetrievalService):\n    \"\"\"\n    A class that performs context retrieval using SQL. The class is responsible for retrieving the top k\n    contexts based on the embedded query. The query is embedded already by the `EmbeddingService`. The class\n    is a wrapper around SQLAlchemy and provides a more testable and maintainable interface. It implements\n    the `RetrievalService` interface.\n\n    Aside from this it is also responsible for storing the token spend and validating the session ID.\n\n    Raises:\n        (InputError): If a Session ID is provided, it must already exist.\n    \"\"\"\n\n    async_session: AsyncSession\n\n    async def retrieve_top_k(\n        self, embedded_query: EmbeddedResponse, k: int\n    ) -&gt; list[RetrievedContext]:\n        \"\"\"\n        Retrieves the top k contexts based on the embedded query. The contexts are the k most relevant\n        documents to the embedded query. The distance between the embedded query and the retrieved context\n        is given by the cosine distance.\n\n        In short, the lower the distance, the more similar the context is to the query.\n        This is why we order by distance and limit the number of contexts to k.\n\n        Args:\n            embedded_query (EmbeddedResponse): The embedded query. This is done by the `EmbeddingService`.\n            k (int): The number of contexts to retrieve.\n                Can be set with the `TOP_K` environment variable. The default value is 5 but it is configurable.\n                Increasing it will also increase the token spend.\n\n        Returns:\n            (list[RetrievedContext]): The list of retrieved contexts.\n        \"\"\"\n\n        stmt = (\n            select(\n                EmbeddedDocumentModel.embedding.cosine_distance(\n                    embedded_query.embedding\n                ).label(\"distance\"),\n                EmbeddedDocumentModel,\n            )\n            .join(\n                GithubFileModel, EmbeddedDocumentModel.document_id == GithubFileModel.id\n            )\n            .order_by(\"distance\")\n            .limit(k)\n        )\n\n        async with self.async_session.begin():\n            with map_errors():\n                result = await self.async_session.execute(stmt)\n                documents = result.all()\n\n        return [\n            RetrievedContext.from_document(doc[0], doc[1].document) for doc in documents\n        ]\n\n    async def store_token_spent(\n        self,\n        session_id: str,\n        token_count: int,\n        model_name: str,\n    ) -&gt; None:\n        \"\"\"\n        In multiple steps of the RAG pipeline, we need to store the token count spent for a given session ID\n        and model name. This is necessary to track the token spend for the given session. This is used further\n        down the line to calculate the current spend and enforce the spend limit.\n\n        The moments where we need to store the token count spent are:\n        - After the embedding step.\n        - After the generation step.\n\n        The moments this is done may also increase as the RAG is further developed.\n\n\n        Args:\n            session_id (str): The session ID. Corresponds to a single conversation.\n            token_count (int): The number of tokens spent.\n            model_name (str): The name of the model used. Different models have different token costs.\n        \"\"\"\n        token_spend = TokenSpendModel(\n            session_id=session_id,\n            token_count=token_count,\n            model=model_name,\n        )\n        async with self.async_session.begin():\n            with map_errors():\n                self.async_session.add(token_spend)\n                await self.async_session.commit()\n\n    async def get_current_spend(self, date: date) -&gt; float:\n        \"\"\"\n        Retrieves the current spend for a given date. The date is mostly given as a parameter to facilitate\n        testing, it keeps this method more pure than it is if it had to instantiate the date.\n\n        The current spend is given as follows:\n        - The token count spent for the given date is retrieved.\n        - The token count spent is multiplied by 0.00001 to get the spend in dollars.\n        - The sum of all the spends is returned.\n\n        0.00001 is taken as a constant, it's the approximate cost, taking into account both model costs. It's a\n        conservative estimate, the actual cost may be lower.\n\n\n        Args:\n            date (datetime.date): The date for which to retrieve the current spend.\n\n        Returns:\n            (float): The current spend for the given date.\n        \"\"\"\n        stmt = select(TokenSpendModel).where(\n            cast(TokenSpendModel.timestamp, Date) == date\n        )\n        async with self.async_session.begin():\n            with map_errors():\n                result = await self.async_session.execute(stmt)\n            return sum([r.token_count for r in result.scalars()]) * 0.00001\n\n    async def validate_session_id(self, session_id: str) -&gt; None:\n        \"\"\"\n        Validates the session ID if provided. This method ensures that the session ID is valid\n        and exists in the database. This is necessary to track the token spend for the given session.\n\n        Args:\n            session_id (str): The unique id of the conversation.\n\n        Raises:\n            (InputError): If a Session ID is provided, it must already exist.\n        \"\"\"\n        stmt = select(TokenSpendModel).where(TokenSpendModel.session_id == session_id)\n        async with self.async_session.begin():\n            with map_errors():\n                result = await self.async_session.execute(stmt)\n            if not result.scalar():\n                raise InputError(\"If a Session ID is provided, it must already exist.\")\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.SQLRetrievalService.get_current_spend","title":"<code>get_current_spend(date)</code>  <code>async</code>","text":"<p>Retrieves the current spend for a given date. The date is mostly given as a parameter to facilitate testing, it keeps this method more pure than it is if it had to instantiate the date.</p> <p>The current spend is given as follows: - The token count spent for the given date is retrieved. - The token count spent is multiplied by 0.00001 to get the spend in dollars. - The sum of all the spends is returned.</p> <p>0.00001 is taken as a constant, it's the approximate cost, taking into account both model costs. It's a conservative estimate, the actual cost may be lower.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>The date for which to retrieve the current spend.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The current spend for the given date.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def get_current_spend(self, date: date) -&gt; float:\n    \"\"\"\n    Retrieves the current spend for a given date. The date is mostly given as a parameter to facilitate\n    testing, it keeps this method more pure than it is if it had to instantiate the date.\n\n    The current spend is given as follows:\n    - The token count spent for the given date is retrieved.\n    - The token count spent is multiplied by 0.00001 to get the spend in dollars.\n    - The sum of all the spends is returned.\n\n    0.00001 is taken as a constant, it's the approximate cost, taking into account both model costs. It's a\n    conservative estimate, the actual cost may be lower.\n\n\n    Args:\n        date (datetime.date): The date for which to retrieve the current spend.\n\n    Returns:\n        (float): The current spend for the given date.\n    \"\"\"\n    stmt = select(TokenSpendModel).where(\n        cast(TokenSpendModel.timestamp, Date) == date\n    )\n    async with self.async_session.begin():\n        with map_errors():\n            result = await self.async_session.execute(stmt)\n        return sum([r.token_count for r in result.scalars()]) * 0.00001\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.SQLRetrievalService.retrieve_top_k","title":"<code>retrieve_top_k(embedded_query, k)</code>  <code>async</code>","text":"<p>Retrieves the top k contexts based on the embedded query. The contexts are the k most relevant documents to the embedded query. The distance between the embedded query and the retrieved context is given by the cosine distance.</p> <p>In short, the lower the distance, the more similar the context is to the query. This is why we order by distance and limit the number of contexts to k.</p> <p>Parameters:</p> Name Type Description Default <code>embedded_query</code> <code>EmbeddedResponse</code> <p>The embedded query. This is done by the <code>EmbeddingService</code>.</p> required <code>k</code> <code>int</code> <p>The number of contexts to retrieve. Can be set with the <code>TOP_K</code> environment variable. The default value is 5 but it is configurable. Increasing it will also increase the token spend.</p> required <p>Returns:</p> Type Description <code>list[RetrievedContext]</code> <p>The list of retrieved contexts.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def retrieve_top_k(\n    self, embedded_query: EmbeddedResponse, k: int\n) -&gt; list[RetrievedContext]:\n    \"\"\"\n    Retrieves the top k contexts based on the embedded query. The contexts are the k most relevant\n    documents to the embedded query. The distance between the embedded query and the retrieved context\n    is given by the cosine distance.\n\n    In short, the lower the distance, the more similar the context is to the query.\n    This is why we order by distance and limit the number of contexts to k.\n\n    Args:\n        embedded_query (EmbeddedResponse): The embedded query. This is done by the `EmbeddingService`.\n        k (int): The number of contexts to retrieve.\n            Can be set with the `TOP_K` environment variable. The default value is 5 but it is configurable.\n            Increasing it will also increase the token spend.\n\n    Returns:\n        (list[RetrievedContext]): The list of retrieved contexts.\n    \"\"\"\n\n    stmt = (\n        select(\n            EmbeddedDocumentModel.embedding.cosine_distance(\n                embedded_query.embedding\n            ).label(\"distance\"),\n            EmbeddedDocumentModel,\n        )\n        .join(\n            GithubFileModel, EmbeddedDocumentModel.document_id == GithubFileModel.id\n        )\n        .order_by(\"distance\")\n        .limit(k)\n    )\n\n    async with self.async_session.begin():\n        with map_errors():\n            result = await self.async_session.execute(stmt)\n            documents = result.all()\n\n    return [\n        RetrievedContext.from_document(doc[0], doc[1].document) for doc in documents\n    ]\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.SQLRetrievalService.store_token_spent","title":"<code>store_token_spent(session_id, token_count, model_name)</code>  <code>async</code>","text":"<p>In multiple steps of the RAG pipeline, we need to store the token count spent for a given session ID and model name. This is necessary to track the token spend for the given session. This is used further down the line to calculate the current spend and enforce the spend limit.</p> <p>The moments where we need to store the token count spent are: - After the embedding step. - After the generation step.</p> <p>The moments this is done may also increase as the RAG is further developed.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The session ID. Corresponds to a single conversation.</p> required <code>token_count</code> <code>int</code> <p>The number of tokens spent.</p> required <code>model_name</code> <code>str</code> <p>The name of the model used. Different models have different token costs.</p> required Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def store_token_spent(\n    self,\n    session_id: str,\n    token_count: int,\n    model_name: str,\n) -&gt; None:\n    \"\"\"\n    In multiple steps of the RAG pipeline, we need to store the token count spent for a given session ID\n    and model name. This is necessary to track the token spend for the given session. This is used further\n    down the line to calculate the current spend and enforce the spend limit.\n\n    The moments where we need to store the token count spent are:\n    - After the embedding step.\n    - After the generation step.\n\n    The moments this is done may also increase as the RAG is further developed.\n\n\n    Args:\n        session_id (str): The session ID. Corresponds to a single conversation.\n        token_count (int): The number of tokens spent.\n        model_name (str): The name of the model used. Different models have different token costs.\n    \"\"\"\n    token_spend = TokenSpendModel(\n        session_id=session_id,\n        token_count=token_count,\n        model=model_name,\n    )\n    async with self.async_session.begin():\n        with map_errors():\n            self.async_session.add(token_spend)\n            await self.async_session.commit()\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.SQLRetrievalService.validate_session_id","title":"<code>validate_session_id(session_id)</code>  <code>async</code>","text":"<p>Validates the session ID if provided. This method ensures that the session ID is valid and exists in the database. This is necessary to track the token spend for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>The unique id of the conversation.</p> required <p>Raises:</p> Type Description <code>InputError</code> <p>If a Session ID is provided, it must already exist.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>async def validate_session_id(self, session_id: str) -&gt; None:\n    \"\"\"\n    Validates the session ID if provided. This method ensures that the session ID is valid\n    and exists in the database. This is necessary to track the token spend for the given session.\n\n    Args:\n        session_id (str): The unique id of the conversation.\n\n    Raises:\n        (InputError): If a Session ID is provided, it must already exist.\n    \"\"\"\n    stmt = select(TokenSpendModel).where(TokenSpendModel.session_id == session_id)\n    async with self.async_session.begin():\n        with map_errors():\n            result = await self.async_session.execute(stmt)\n        if not result.scalar():\n            raise InputError(\"If a Session ID is provided, it must already exist.\")\n</code></pre>"},{"location":"backend/reference/retrieve/#backend.retrieval_augmented_generation.retrieve.add_sources","title":"<code>add_sources(answer, retrieved)</code>","text":"<p>Adds the sources to the answer. The sources are the list of retrieved contexts. The sources are added as an HTML section at the end of the answer. Args:     answer (str): The answer generated by the model.     retrieved (list[RetrievedContext]): The list of retrieved contexts. Returns:     (str): The answer with the sources added as an HTML section at the end.</p> Source code in <code>backend/backend/retrieval_augmented_generation/retrieve.py</code> <pre><code>def add_sources(answer: str, retrieved: list[RetrievedContext]) -&gt; str:\n    \"\"\"\n    Adds the sources to the answer. The sources are the list of retrieved contexts.\n    The sources are added as an HTML section at the end of the answer.\n    Args:\n        answer (str): The answer generated by the model.\n        retrieved (list[RetrievedContext]): The list of retrieved contexts.\n    Returns:\n        (str): The answer with the sources added as an HTML section at the end.\n    \"\"\"\n    sources = [\n        f\"&lt;li&gt;{escape(r.file_name)} in {escape(r.repository_name)}, &lt;a href={escape(r.url)}&gt;source.&lt;/a&gt;&lt;/li&gt;\"\n        for r in retrieved\n    ]\n    return f\"\"\"{answer}\\n &lt;section id=\"sources\"&gt;To answer this question, I used the following sources:\n        &lt;ul&gt;{''.join(sources)}&lt;/ul&gt;&lt;/section&gt;\"\"\"\n</code></pre>"},{"location":"backend/reference/tests/fixtures/","title":"Fixtures","text":""},{"location":"backend/reference/tests/fixtures/#tests.integration.database_fixtures.add_fake_data","title":"<code>add_fake_data(database_session)</code>  <code>async</code>","text":"<p>Adds fake data to the database for testing purposes. The data consists of a GitHub repository with two files, each with an embedded document.</p> <p>Parameters:</p> Name Type Description Default <code>database_session</code> <code>async_sessionmaker[AsyncSession]</code> <p>This is the fixture that provides the database session. It spins up a Postgres container and creates a session for testing.</p> required Source code in <code>backend/tests/integration/database_fixtures.py</code> <pre><code>@pytest_asyncio.fixture(scope=\"session\")\nasync def add_fake_data(database_session: async_sessionmaker[AsyncSession]) -&gt; None:\n    \"\"\"\n    Adds fake data to the database for testing purposes. The data consists of a\n    GitHub repository with two files, each with an embedded document.\n\n\n    Args:\n        database_session (async_sessionmaker[AsyncSession]): This is the fixture that\n            provides the database session. It spins up a Postgres container and creates\n            a session for testing.\n    \"\"\"\n    repository = GitHubRepositoryModel(\n        name=\"repo1\",\n        user=\"user1\",\n        url=\"http://repo1.com\",\n    )\n    file1 = GithubFileModel(\n        name=\"file1\",\n        content_url=\"http://file1.com\",\n        last_modified=datetime.now(),\n        repository_name=\"repo1\",\n        repository_user=\"user1\",\n        file_extension=\"py\",\n        path_in_repo=\"path/to/file1.py\",\n        latest_version=True,\n        is_embedded=False,\n    )\n    file2 = GithubFileModel(\n        name=\"file2\",\n        content_url=\"http://file2.com\",\n        last_modified=datetime.now(),\n        repository_name=\"repo1\",\n        repository_user=\"user1\",\n        file_extension=\"py\",\n        path_in_repo=\"path/to/file2.py\",\n        latest_version=True,\n        is_embedded=False,\n    )\n    embedded_document1 = EmbeddedDocumentModel(\n        document=file1,\n        embedding=np.random.rand(3072).tolist(),\n        input_token_count=100,\n    )\n    embedded_document2 = EmbeddedDocumentModel(\n        document=file2,\n        embedding=np.random.rand(3072).tolist(),\n        input_token_count=100,\n    )\n    async with database_session() as session:\n        file1.embedding.append(embedded_document1)\n        file2.embedding.append(embedded_document2)\n        repository.files.extend([file1, file2])\n        session.add(repository)\n        await session.commit()\n</code></pre>"},{"location":"backend/reference/tests/fixtures/#tests.integration.database_fixtures.database_session","title":"<code>database_session()</code>","text":"<p>Starts a Postgres container and creates a database session for testing. The scope of the fixture is session-wide, meaning that the container will be started once and the session will be created once for all tests. All tests are thus sharing the same database and in the current setup also share the same event loop.</p> <p>Yields:</p> Type Description <code>Generator[async_sessionmaker[AsyncSession], None, None]</code> <p>The database session generator.</p> Source code in <code>backend/tests/integration/database_fixtures.py</code> <pre><code>@pytest.fixture(scope=\"session\")\ndef database_session() -&gt; Generator[async_sessionmaker[AsyncSession], None, None]:\n    \"\"\"\n    Starts a Postgres container and creates a database session for testing.\n    The scope of the fixture is session-wide, meaning that the container will be\n    started once and the session will be created once for all tests. All tests\n    are thus sharing the same database and in the current setup also share the\n    same event loop.\n\n    Yields:\n        (Generator[async_sessionmaker[AsyncSession], None, None]): The database session generator.\n    \"\"\"\n    with PostgresContainer(\n        \"pgvector/pgvector:0.7.0-pg16\",\n        username=\"postgres\",\n        password=\"postgres\",\n        dbname=\"chatGITpt\",\n        driver=\"asyncpg\",\n    ) as postgres:\n        conn_str = postgres.get_connection_url()\n        sync_conn_str = conn_str.replace(\"asyncpg\", \"psycopg2\")\n        os.environ[\"DATABASE_URL\"] = sync_conn_str\n        run_migrations(\"../shared/shared/migrations\", logger)\n        engine = create_async_engine(conn_str)\n        Session = async_sessionmaker(engine, expire_on_commit=False)\n        yield Session\n</code></pre>"},{"location":"backend/reference/tests/fixtures/#tests.integration.database_fixtures.retrieval_service","title":"<code>retrieval_service(database_session, add_fake_data)</code>  <code>async</code>","text":"<p>A fixture that provides a <code>SQLRetrievalService</code> instance for testing. It uses the database session to connect to the database and adds fake data to the database for testing purposes. It is a session-scoped fixture, meaning that the database session is created once for all tests and the fake data is added once for all tests.</p> <p>Parameters:</p> Name Type Description Default <code>database_session</code> <code>async_sessionmaker[AsyncSession]</code> <p>This is the fixture that provides the database session. It spins up a Postgres container and creates a session for testing.</p> required <code>add_fake_data</code> <code>None</code> <p>This is the fixture that adds fake data to the database for testing purposes.</p> required <p>Yields:</p> Type Description <code> AsyncGenerator[SQLRetrievalService, None]</code> <p>The retrieval service instance.</p> Source code in <code>backend/tests/integration/database_fixtures.py</code> <pre><code>@pytest_asyncio.fixture(scope=\"session\")\nasync def retrieval_service(\n    database_session: async_sessionmaker[AsyncSession],\n    add_fake_data: None,\n) -&gt; AsyncGenerator[SQLRetrievalService, None]:\n    \"\"\"\n    A fixture that provides a `SQLRetrievalService` instance for testing. It uses the\n    database session to connect to the database and adds fake data to the database for\n    testing purposes. It is a session-scoped fixture, meaning that the database session\n    is created once for all tests and the fake data is added once for all tests.\n\n    Args:\n        database_session (async_sessionmaker[AsyncSession]): This is the fixture that\n            provides the database session. It spins up a Postgres container and creates\n            a session for testing.\n        add_fake_data (None): This is the fixture that adds fake data to the database\n            for testing purposes.\n\n    Yields:\n       ( AsyncGenerator[SQLRetrievalService, None]): The retrieval service instance.\n    \"\"\"\n\n    async with database_session() as session:\n\n        service = SQLRetrievalService(session)\n        yield service\n</code></pre>"},{"location":"frontend/overview/","title":"Overview","text":"<p>The frontend consists of a SvelteKit application. The actual chat application is a mostly a single page application (SPA) that. It responsible for rendering the chat interface, sending and receiving messages, and managing the user's session.  The remainder are static pages that provide information about the project.</p>"},{"location":"pipelines/Overview/","title":"Overview","text":"<p>The data pipelines are orchestrated with dagster. The code is set up in a way to have minimal dependencies on the framework itself. This is to ensure that the code can be easily ported to other frameworks if needed. The pipelines are defined in the <code>pipelines</code> directory.</p>"},{"location":"pipelines/reference/assets/","title":"Assets","text":""},{"location":"pipelines/reference/assets/#pipelines.orchestration.assets.get_session","title":"<code>get_session(app)</code>","text":"<p>Create a sessionmaker for the database connection.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>AppConfig</code> <p>The application configuration.</p> required <p>Returns:</p> Type Description <code>sessionmaker[Session]</code> <p>sessionmaker[Session]: The sessionmaker for the database connection.</p> Source code in <code>pipelines/pipelines/orchestration/assets.py</code> <pre><code>def get_session(app: AppConfig) -&gt; sessionmaker[Session]:\n    \"\"\"\n    Create a sessionmaker for the database connection.\n\n    Args:\n        app (AppConfig): The application configuration.\n        Contains the database connection string.\n\n    Returns:\n        sessionmaker[Session]: The sessionmaker for the database connection.\n    \"\"\"\n    engine = create_engine(app.db_connection_string, echo=True)\n    Session = sessionmaker(engine)\n    return Session\n</code></pre>"},{"location":"pipelines/reference/assets/#pipelines.orchestration.assets.persist_data","title":"<code>persist_data(app_config_resource)</code>","text":"<p>Persist data from the GitHub API to the database.</p> <p>Parameters:</p> Name Type Description Default <code>app_config_resource</code> <code>AppConfigResource</code> <p>The application configuration.</p> required Source code in <code>pipelines/pipelines/orchestration/assets.py</code> <pre><code>@asset\ndef persist_data(app_config_resource: AppConfigResource) -&gt; None:\n    \"\"\"\n    Persist data from the GitHub API to the database.\n\n    Args:\n        app_config_resource (AppConfigResource): The application configuration.\n        It is a resource class because the dagster framework requires\n        it to be so.\n    \"\"\"\n    app_config = app_config_resource.get_app_config()\n    client = app_config.get_github_client()\n    Session = get_session(app_config)\n    db_service = DatabaseService(Session)\n    ingestion_service = IngestionService(db_service, client)\n    ingestion_service.fetch_and_persist_data()\n</code></pre>"},{"location":"pipelines/reference/assets/#pipelines.orchestration.assets.persist_embeddings","title":"<code>persist_embeddings(app_config_resource)</code>  <code>async</code>","text":"<p>Persist embeddings of the files in the database. This function depends on the <code>persist_data</code> asset to run first.</p> <p>Parameters:</p> Name Type Description Default <code>app_config_resource</code> <code>AppConfigResource</code> <p>The application configuration.</p> required Source code in <code>pipelines/pipelines/orchestration/assets.py</code> <pre><code>@asset(deps=[persist_data])\nasync def persist_embeddings(app_config_resource: AppConfigResource) -&gt; None:\n    \"\"\"\n    Persist embeddings of the files in the database.\n    This function depends on the `persist_data` asset to run first.\n\n    Args:\n        app_config_resource (AppConfigResource): The application configuration.\n        It is a resource class because the dagster framework requires it to be so.\n    \"\"\"\n    app_config = app_config_resource.get_app_config()\n    openai_client = app_config.get_openai_client()\n    Session = get_session(app_config)\n    github_token = app_config.github_token\n    auth_header = AuthHeader(Authorization=\"Authorization\", token=github_token)\n    persistance = EmbeddingPersistence(Session)\n    embedder = OpenAIEmbedder(openai_client, app_config.embedding_model)\n    embedding_service = EmbeddingService(\n        persistance,\n        embedder,\n        auth_header,\n        app_config.blacklisted_files,\n        app_config.whitelisted_extensions,\n    )\n\n    await embedding_service.embed_and_persist_files()\n</code></pre>"},{"location":"pipelines/reference/embedding/","title":"Embedding","text":""},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.AuthHeader","title":"<code>AuthHeader</code>  <code>dataclass</code>","text":"<p>Small dataclass to store the authorization header for the API requests. Necessary for the requests to the GitHub API.</p> <p>Parameters:</p> Name Type Description Default <code>Authorization</code> <code>str</code> <p>The name of the authorization header. (for example <code>Authorization</code>)</p> required <code>token</code> <code>str</code> <p>The token to be used for authorization. This is the GitHub token, in this case you need to request a classic GitHub token.</p> required Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass AuthHeader:\n    \"\"\"\n    Small dataclass to store the authorization header for the API requests.\n    Necessary for the requests to the GitHub API.\n\n    Args:\n        Authorization (str): The name of the authorization header. (for example `Authorization`)\n        token (str): The token to be used for authorization. This is the GitHub token, in this case\n            you need to request a classic GitHub token.\n    \"\"\"\n\n    Authorization: str\n    token: str\n\n    def to_dict(self) -&gt; dict[str, str]:\n        \"\"\"Returns the authorization header as a dictionary.\n        The dictionary is used to pass the authorization header to the aiohttp library. This is necessary\n        for the requests to the GitHub API.\n\n        Returns:\n            dict[str, str]: The authorization header as a dictionary.\n        \"\"\"\n        return {\"Authorization\": f\"Bearer {self.token}\"}\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.AuthHeader.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns the authorization header as a dictionary. The dictionary is used to pass the authorization header to the aiohttp library. This is necessary for the requests to the GitHub API.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: The authorization header as a dictionary.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>def to_dict(self) -&gt; dict[str, str]:\n    \"\"\"Returns the authorization header as a dictionary.\n    The dictionary is used to pass the authorization header to the aiohttp library. This is necessary\n    for the requests to the GitHub API.\n\n    Returns:\n        dict[str, str]: The authorization header as a dictionary.\n    \"\"\"\n    return {\"Authorization\": f\"Bearer {self.token}\"}\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingPersistence","title":"<code>EmbeddingPersistence</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EmbeddingStore</code></p> <p>A class that implements the EmbeddingStore protocol. It is used to store the embeddings in the database as well as retrieve the metadata of the files that are to be embedded.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass EmbeddingPersistence(EmbeddingStore):\n    \"\"\"\n    A class that implements the EmbeddingStore protocol.\n    It is used to store the embeddings in the database as well as retrieve the\n    metadata of the files that are to be embedded.\n    \"\"\"\n\n    session_maker: sessionmaker[Session]\n\n    def find_files(\n        self,\n        white_list: list[str],\n        blacklisted_files: list[str],\n    ) -&gt; list[FileMetadata]:\n        query = (\n            select(GithubFileModel)\n            .where(GithubFileModel.is_embedded.is_(False))\n            .where(GithubFileModel.file_extension.in_(white_list))\n            .where(GithubFileModel.name.not_in(blacklisted_files))\n        )\n        with self.session_maker() as session:\n            files = session.scalars(query).all()\n        app_logger.info(f\"Found {len(files)} files to embed.\")\n        github_files = [FileMetadata.from_db_object(file) for file in files]\n        return github_files\n\n    def save_embeddings(\n        self, embeddings: EmbeddingWithCount, metadata: FileMetadata\n    ) -&gt; None:\n        embedded_document = EmbeddedDocumentModel(\n            document_id=metadata.document_id,\n            embedding=embeddings.embedding,\n            input_token_count=embeddings.total_tokens,\n        )\n        original_file = select(GithubFileModel).where(\n            GithubFileModel.id == metadata.document_id\n        )\n        with self.session_maker() as session:\n            orig = session.execute(original_file).scalar()\n            if orig:\n                orig.is_embedded = True\n            session.add(embedded_document)\n            session.commit()\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingService","title":"<code>EmbeddingService</code>  <code>dataclass</code>","text":"<p>This class is used to embed files and save the embeddings to the database.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>EmbeddingStore</code> <p>The database object that is used to store the embeddings.</p> required <code>embedder</code> <code>TextEmbedder</code> <p>The object that is used to embed the files.</p> required <code>auth_header</code> <code>AuthHeader</code> <p>The authorization header for the API requests.</p> required <code>blacklisted_files</code> <code>list[str]</code> <p>The list of file names that are not allowed to be embedded.</p> required <code>white_list</code> <code>list[str]</code> <p>The list of file extensions that are allowed to be embedded.</p> required <code>splitter</code> <code>TextSplitter</code> <p>The object that is used to split the text into chunks.</p> <code>TextSplitter()</code> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass EmbeddingService:\n    \"\"\"\n    This class is used to embed files and save the embeddings to the database.\n\n    Args:\n        db (EmbeddingStore): The database object that is used to store the embeddings.\n        embedder (TextEmbedder): The object that is used to embed the files.\n        auth_header (AuthHeader): The authorization header for the API requests.\n        blacklisted_files (list[str]): The list of file names that are not allowed to be embedded.\n        white_list (list[str]): The list of file extensions that are allowed to be embedded.\n        splitter (TextSplitter): The object that is used to split the text into chunks.\n    \"\"\"\n\n    db: EmbeddingStore\n    embedder: TextEmbedder\n    auth_header: AuthHeader\n    blacklisted_files: list[str]\n    white_list: list[str]\n    splitter: TextSplitter = TextSplitter()\n\n    async def embed_and_persist_files(self) -&gt; None:\n        \"\"\"\n        Embeds the files and saves the embeddings to the database.\n        We do this in two steps:\n        1. Fetch the files from the GitHub API.\n        2. Embed the files and save the embeddings to the\n        database.\n        \"\"\"\n\n        metadata = self.db.find_files(self.white_list, self.blacklisted_files)\n        file_futures = [self.get_file_content(meta) for meta in metadata]\n\n        file_contents = await asyncio.gather(*file_futures)\n        app_logger.info(\"Fetched all files. Starting to embed.\")\n        embedding_futures = [\n            self.process_and_save(text, metadata)\n            for metadata, text in zip(metadata, file_contents)\n        ]\n\n        _ = await asyncio.gather(*embedding_futures)\n\n    async def get_file_content(self, metadata: FileMetadata) -&gt; str:\n        \"\"\"\n        Fetches the file content from the GitHub API.\n        The database doesn't store the content of the file,\n        so we need to fetch it from the GitHub API.\n\n        Args:\n            metadata (FileMetadata): The metadata of the file to fetch.\n\n        Returns:\n            str: The code that is stored in the file.\n        \"\"\"\n        app_logger.debug(\n            f\"Fetching document {metadata.document_id} from {metadata.repository_name}\"\n        )\n        path = metadata.file.content_url\n        async with aiohttp.ClientSession(headers=self.auth_header.to_dict()) as session:\n            async with session.get(path) as response:\n                return await response.text()\n\n    async def process_and_save(\n        self,\n        text: str,\n        metadata: FileMetadata,\n    ) -&gt; None:\n        \"\"\"\n        Embeds the code and saves the embeddings to the database.\n        The metadata is also added while saving the embeddings to add things\n        like the file name, extensions, path, repository and so on.\n\n        Args:\n            text (str): The code to embed.\n            metadata (FileMetadata): The metadata of the file.\n        \"\"\"\n        chunks = self.splitter.split_text_to_chunks(text, \"test\")\n        embeddings = await self.embedder.embed_chunk(chunks, metadata)\n        for embedding in embeddings:\n            self.db.save_embeddings(embedding, metadata)\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingService.embed_and_persist_files","title":"<code>embed_and_persist_files()</code>  <code>async</code>","text":"<p>Embeds the files and saves the embeddings to the database. We do this in two steps: 1. Fetch the files from the GitHub API. 2. Embed the files and save the embeddings to the database.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>async def embed_and_persist_files(self) -&gt; None:\n    \"\"\"\n    Embeds the files and saves the embeddings to the database.\n    We do this in two steps:\n    1. Fetch the files from the GitHub API.\n    2. Embed the files and save the embeddings to the\n    database.\n    \"\"\"\n\n    metadata = self.db.find_files(self.white_list, self.blacklisted_files)\n    file_futures = [self.get_file_content(meta) for meta in metadata]\n\n    file_contents = await asyncio.gather(*file_futures)\n    app_logger.info(\"Fetched all files. Starting to embed.\")\n    embedding_futures = [\n        self.process_and_save(text, metadata)\n        for metadata, text in zip(metadata, file_contents)\n    ]\n\n    _ = await asyncio.gather(*embedding_futures)\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingService.get_file_content","title":"<code>get_file_content(metadata)</code>  <code>async</code>","text":"<p>Fetches the file content from the GitHub API. The database doesn't store the content of the file, so we need to fetch it from the GitHub API.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>FileMetadata</code> <p>The metadata of the file to fetch.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The code that is stored in the file.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>async def get_file_content(self, metadata: FileMetadata) -&gt; str:\n    \"\"\"\n    Fetches the file content from the GitHub API.\n    The database doesn't store the content of the file,\n    so we need to fetch it from the GitHub API.\n\n    Args:\n        metadata (FileMetadata): The metadata of the file to fetch.\n\n    Returns:\n        str: The code that is stored in the file.\n    \"\"\"\n    app_logger.debug(\n        f\"Fetching document {metadata.document_id} from {metadata.repository_name}\"\n    )\n    path = metadata.file.content_url\n    async with aiohttp.ClientSession(headers=self.auth_header.to_dict()) as session:\n        async with session.get(path) as response:\n            return await response.text()\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingService.process_and_save","title":"<code>process_and_save(text, metadata)</code>  <code>async</code>","text":"<p>Embeds the code and saves the embeddings to the database. The metadata is also added while saving the embeddings to add things like the file name, extensions, path, repository and so on.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The code to embed.</p> required <code>metadata</code> <code>FileMetadata</code> <p>The metadata of the file.</p> required Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>async def process_and_save(\n    self,\n    text: str,\n    metadata: FileMetadata,\n) -&gt; None:\n    \"\"\"\n    Embeds the code and saves the embeddings to the database.\n    The metadata is also added while saving the embeddings to add things\n    like the file name, extensions, path, repository and so on.\n\n    Args:\n        text (str): The code to embed.\n        metadata (FileMetadata): The metadata of the file.\n    \"\"\"\n    chunks = self.splitter.split_text_to_chunks(text, \"test\")\n    embeddings = await self.embedder.embed_chunk(chunks, metadata)\n    for embedding in embeddings:\n        self.db.save_embeddings(embedding, metadata)\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingStore","title":"<code>EmbeddingStore</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>This class is used to store embeddings as well as retrieve file metadata required for the embedding process.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>class EmbeddingStore(Protocol):\n    \"\"\"\n    This class is used to store embeddings as well as retrieve file metadata required for the embedding process.\n    \"\"\"\n\n    def find_files(\n        self,\n        white_list: list[str],\n        blacklisted_files: list[str],\n    ) -&gt; list[FileMetadata]:\n        \"\"\"\n        Finds the files that are not yet embedded and are in the white list and not in the black list.\n        The reason is that we use ELT instead of ETL, all files are ingested and then filtered\n        out based on the white list and black list.\n\n        Args:\n            white_list (list[str]): The list of file extensions that are allowed to be embedded.\n            blacklisted_files (list[str]): The list of file names that are not allowed to be embedded.\n\n        Returns:\n            (list[FileMetadata]): The list of files that are allowed to be embedded.\n        \"\"\"\n\n        ...\n\n    def save_embeddings(\n        self, embeddings: EmbeddingWithCount, metadata: FileMetadata\n    ) -&gt; None:\n        \"\"\"\n        Saves the embeddings to the database.\n\n        Args:\n            embeddings (EmbeddingWithCount): The embeddings to save.\n            metadata (FileMetadata): The metadata of the file that was embedded.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingStore.find_files","title":"<code>find_files(white_list, blacklisted_files)</code>","text":"<p>Finds the files that are not yet embedded and are in the white list and not in the black list. The reason is that we use ELT instead of ETL, all files are ingested and then filtered out based on the white list and black list.</p> <p>Parameters:</p> Name Type Description Default <code>white_list</code> <code>list[str]</code> <p>The list of file extensions that are allowed to be embedded.</p> required <code>blacklisted_files</code> <code>list[str]</code> <p>The list of file names that are not allowed to be embedded.</p> required <p>Returns:</p> Type Description <code>list[FileMetadata]</code> <p>The list of files that are allowed to be embedded.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>def find_files(\n    self,\n    white_list: list[str],\n    blacklisted_files: list[str],\n) -&gt; list[FileMetadata]:\n    \"\"\"\n    Finds the files that are not yet embedded and are in the white list and not in the black list.\n    The reason is that we use ELT instead of ETL, all files are ingested and then filtered\n    out based on the white list and black list.\n\n    Args:\n        white_list (list[str]): The list of file extensions that are allowed to be embedded.\n        blacklisted_files (list[str]): The list of file names that are not allowed to be embedded.\n\n    Returns:\n        (list[FileMetadata]): The list of files that are allowed to be embedded.\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingStore.save_embeddings","title":"<code>save_embeddings(embeddings, metadata)</code>","text":"<p>Saves the embeddings to the database.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>EmbeddingWithCount</code> <p>The embeddings to save.</p> required <code>metadata</code> <code>FileMetadata</code> <p>The metadata of the file that was embedded.</p> required Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>def save_embeddings(\n    self, embeddings: EmbeddingWithCount, metadata: FileMetadata\n) -&gt; None:\n    \"\"\"\n    Saves the embeddings to the database.\n\n    Args:\n        embeddings (EmbeddingWithCount): The embeddings to save.\n        metadata (FileMetadata): The metadata of the file that was embedded.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.EmbeddingWithCount","title":"<code>EmbeddingWithCount</code>  <code>dataclass</code>","text":"<p>This class is used to store the embeddings of a file as well as the total number of tokens in the file. We store the total number of tokens in the file so that we can calculate the cost of the embedding.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>list[float]</code> <p>The embeddings of the file.</p> required <code>total_tokens</code> <code>int</code> <p>The total number of tokens in the file.</p> required Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass EmbeddingWithCount:\n    \"\"\"\n    This class is used to store the embeddings of a file as well as the total number of tokens in the file.\n    We store the total number of tokens in the file so that we can calculate the cost of the embedding.\n\n    Args:\n        embedding (list[float]): The embeddings of the file.\n        total_tokens (int): The total number of tokens in the file.\n    \"\"\"\n\n    embedding: list[float]\n    total_tokens: int\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.FileMetadata","title":"<code>FileMetadata</code>  <code>dataclass</code>","text":"<p>This class is used to store metadata about the files that are to be embedded.</p> <p>Parameters:</p> Name Type Description Default <code>repository_name</code> <code>str</code> <p>The name of the repository that the file belongs to.</p> required <code>document_id</code> <code>int</code> <p>The id of the document in the database.</p> required <code>file</code> <code>GitHubFile</code> <p>The file object that contains metadata about the file.</p> required Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass FileMetadata:\n    \"\"\"\n    This class is used to store metadata about the files that are to be embedded.\n\n    Args:\n        repository_name (str): The name of the repository that the file belongs to.\n        document_id (int): The id of the document in the database.\n        file (GitHubFile): The file object that contains metadata about the file.\n    \"\"\"\n\n    repository_name: str\n    document_id: int\n    file: GitHubFile\n\n    @classmethod\n    def from_db_object(cls, file: GithubFileModel) -&gt; Self:\n        \"\"\"\n        Creates a FileMetadata object from a GithubFileModel object.\n        The point is to convert the database object into a domain object.\n\n\n        Args:\n            file (GithubFileModel): The database object to convert.\n\n        Returns:\n            Self: The domain object.\n        \"\"\"\n        return cls(\n            repository_name=file.repository_name,\n            document_id=file.id,\n            file=GitHubFile.from_db_object(file),\n        )\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.FileMetadata.from_db_object","title":"<code>from_db_object(file)</code>  <code>classmethod</code>","text":"<p>Creates a FileMetadata object from a GithubFileModel object. The point is to convert the database object into a domain object.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>GithubFileModel</code> <p>The database object to convert.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The domain object.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>@classmethod\ndef from_db_object(cls, file: GithubFileModel) -&gt; Self:\n    \"\"\"\n    Creates a FileMetadata object from a GithubFileModel object.\n    The point is to convert the database object into a domain object.\n\n\n    Args:\n        file (GithubFileModel): The database object to convert.\n\n    Returns:\n        Self: The domain object.\n    \"\"\"\n    return cls(\n        repository_name=file.repository_name,\n        document_id=file.id,\n        file=GitHubFile.from_db_object(file),\n    )\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.OpenAIEmbedder","title":"<code>OpenAIEmbedder</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TextEmbedder</code></p> <p>A TextEmbedder implementation that uses the OpenAI API to embed text. Args:     api_client (AsyncOpenAI): The OpenAI API client to use.     embedding_model (str): The name of the embedding model to use.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass OpenAIEmbedder(TextEmbedder):\n    \"\"\"\n    A TextEmbedder implementation that uses the OpenAI API to embed text.\n    Args:\n        api_client (AsyncOpenAI): The OpenAI API client to use.\n        embedding_model (str): The name of the embedding model to use.\n    \"\"\"\n\n    api_client: AsyncOpenAI\n    embedding_model: str\n\n    async def embed_chunk(\n        self,\n        chunks: list[str],\n        metadata: FileMetadata,\n    ) -&gt; list[EmbeddingWithCount]:\n        enriched_content = [\n            self._enrich_file_content(chunk, metadata.file) for chunk in chunks\n        ]\n        embeddings = self._embed_document(enriched_content)\n        return await embeddings\n\n    def _enrich_file_content(self, file_content: str, file: GitHubFile) -&gt; str:\n        file_name = f\"\\nThe file name is {file.name}.\\n\"\n        file_place_in_project = f\"The file is located at {file.path_in_project}.\\n\"\n        file_extension = f\"The file extension is {file.extension}.\\n\"\n        return file_content + file_name + file_place_in_project + file_extension\n\n    async def _embed_document(self, text: list[str]) -&gt; list[EmbeddingWithCount]:\n        return await self._embed_documents_with_retry(text, 3)\n\n    async def _embed_documents_with_retry(\n        self,\n        text: list[str],\n        max_retries: int,\n    ) -&gt; list[EmbeddingWithCount]:\n        for attempt in range(max_retries):\n            futures: list[Coroutine[Any, Any, CreateEmbeddingResponse]] = []\n            try:\n                for chunk in text:\n                    future = self.api_client.embeddings.create(\n                        model=self.embedding_model, input=chunk\n                    )\n                    futures.append(future)\n\n                responses = await asyncio.gather(*futures)\n                embeddings = self._process_responses(responses)\n                return embeddings\n\n            except Exception as e:\n                app_logger.error(f\"Received an internal server error. {attempt}s left\")\n                if attempt == max_retries - 1:\n                    raise e\n                await asyncio.sleep(5 * attempt + 1)\n        else:\n            raise RuntimeError(\"Failed to retrieve embeddings after maximum retries.\")\n\n    def _process_responses(self, responses: list[CreateEmbeddingResponse]):\n        embeddings: list[EmbeddingWithCount] = []\n        for response in responses:\n            embedding_vec = response.data[0].embedding\n            tokens = response.usage.total_tokens\n            embedding = EmbeddingWithCount(embedding_vec, tokens)\n            embeddings.append(embedding)\n        return embeddings\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.TextEmbedder","title":"<code>TextEmbedder</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>This class is used to embed text, it takes text and metadata and returns the embedding and the total number of tokens in the text.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>class TextEmbedder(Protocol):\n    \"\"\"\n    This class is used to embed text, it takes text and metadata and returns the embedding\n    and the total number of tokens in the text.\n    \"\"\"\n\n    async def embed_chunk(\n        self,\n        chunks: list[str],\n        metadata: FileMetadata,\n    ) -&gt; list[EmbeddingWithCount]:\n        \"\"\"\n        Embeds the text and returns the embeddings and the total number of tokens in the text.\n\n        Args:\n            text (str): The code to embed.\n            metadata (FileMetadata): The metadata of the file.\n\n        Returns:\n            list[EmbeddingWithCount]: The embeddings and the total number of tokens in the text.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.TextEmbedder.embed_chunk","title":"<code>embed_chunk(chunks, metadata)</code>  <code>async</code>","text":"<p>Embeds the text and returns the embeddings and the total number of tokens in the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The code to embed.</p> required <code>metadata</code> <code>FileMetadata</code> <p>The metadata of the file.</p> required <p>Returns:</p> Type Description <code>list[EmbeddingWithCount]</code> <p>list[EmbeddingWithCount]: The embeddings and the total number of tokens in the text.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>async def embed_chunk(\n    self,\n    chunks: list[str],\n    metadata: FileMetadata,\n) -&gt; list[EmbeddingWithCount]:\n    \"\"\"\n    Embeds the text and returns the embeddings and the total number of tokens in the text.\n\n    Args:\n        text (str): The code to embed.\n        metadata (FileMetadata): The metadata of the file.\n\n    Returns:\n        list[EmbeddingWithCount]: The embeddings and the total number of tokens in the text.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.TextSplitter","title":"<code>TextSplitter</code>  <code>dataclass</code>","text":"<p>This class is used to split the text into chunks of 7000 tokens. This is necessary because the OpenAI API has a limit of 8000 tokens per request. We split the text into chunks of 7000 tokens to account for the tokens used by the metadata.</p> <p>Parameters:</p> Name Type Description Default <code>encoding</code> <code>Encoding</code> <p>The encoding to use for tokenization.</p> <code>get_encoding('cl100k_base')</code> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass TextSplitter:\n    \"\"\"\n    This class is used to split the text into chunks of 7000 tokens.\n    This is necessary because the OpenAI API has a limit of 8000 tokens per request.\n    We split the text into chunks of 7000 tokens to account for the tokens used by the metadata.\n\n    Args:\n        encoding (Encoding): The encoding to use for tokenization.\n    \"\"\"\n\n    encoding: Encoding = tiktoken.get_encoding(\"cl100k_base\")\n\n    def split_text_to_chunks(self, text: str, name: str) -&gt; list[str]:\n        \"\"\"\n        Splits the text into chunks of 7000 tokens.\n\n        Args:\n            text (str): The text to split.\n            name (str): The name of the file. Used for logging.\n\n        Returns:\n            list[str]: The list of chunks.\n        \"\"\"\n        tokens = self._count_tokens(text)\n        num_chunks = (tokens // 7000) + 1\n        char_per_chunk = len(text) // num_chunks\n        app_logger.info(\n            f\"Splitting {name} into {num_chunks} chunks of {char_per_chunk} characters.\"\n        )\n        return [\n            text[i : i + char_per_chunk]  # noqa E203\n            for i in range(0, len(text), char_per_chunk + 1)\n            # + 1 is added for empty files\n        ]\n\n    def _count_tokens(self, text: str) -&gt; int:\n        num_tokens = len(self.encoding.encode(text))\n        return num_tokens\n</code></pre>"},{"location":"pipelines/reference/embedding/#pipelines.processing.embedding.TextSplitter.split_text_to_chunks","title":"<code>split_text_to_chunks(text, name)</code>","text":"<p>Splits the text into chunks of 7000 tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split.</p> required <code>name</code> <code>str</code> <p>The name of the file. Used for logging.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The list of chunks.</p> Source code in <code>pipelines/pipelines/processing/embedding.py</code> <pre><code>def split_text_to_chunks(self, text: str, name: str) -&gt; list[str]:\n    \"\"\"\n    Splits the text into chunks of 7000 tokens.\n\n    Args:\n        text (str): The text to split.\n        name (str): The name of the file. Used for logging.\n\n    Returns:\n        list[str]: The list of chunks.\n    \"\"\"\n    tokens = self._count_tokens(text)\n    num_chunks = (tokens // 7000) + 1\n    char_per_chunk = len(text) // num_chunks\n    app_logger.info(\n        f\"Splitting {name} into {num_chunks} chunks of {char_per_chunk} characters.\"\n    )\n    return [\n        text[i : i + char_per_chunk]  # noqa E203\n        for i in range(0, len(text), char_per_chunk + 1)\n        # + 1 is added for empty files\n    ]\n</code></pre>"},{"location":"pipelines/reference/ingestion/","title":"Ingestion","text":""},{"location":"pipelines/reference/resources/","title":"Resources","text":""},{"location":"pipelines/reference/resources/#pipelines.orchestration.resources.AppConfigResource","title":"<code>AppConfigResource</code>","text":"<p>               Bases: <code>ConfigurableResource</code></p> <p>Resource class for the application configuration. It is used by the Dagster framework to inject the application configuration into the asset functions.</p> <p>Parameters:</p> Name Type Description Default <code>github_token</code> <code>str</code> <p>The GitHub API token.</p> required <code>openai_api_key</code> <code>str</code> <p>The OpenAI API key.</p> required <code>db_connection_string</code> <code>str</code> <p>The database connection string.</p> required <code>whitelisted_extensions</code> <code>list[str]</code> <p>The list of whitelisted file extensions. These extensions will be used to filter the files fetched from the GitHub API.</p> required <code>blacklisted_files</code> <code>list[str]</code> <p>The list of blacklisted file names. These files will be ignored when fetching data from the GitHub API.</p> required <code>embedding_disk_path</code> <code>str</code> <p>The path to the disk where the embeddings are stored. The embeddings are temporarily stored on disk before being persisted to the database.</p> required <code>embedding_model</code> <code>str</code> <p>The OpenAI model used for text embedding.</p> required <code>max_embedding_input_length</code> <code>int</code> <p>The maximum length of the input text for embedding.</p> required Source code in <code>pipelines/pipelines/orchestration/resources.py</code> <pre><code>class AppConfigResource(ConfigurableResource):\n    \"\"\"\n    Resource class for the application configuration. It is used by the Dagster\n    framework to inject the application configuration into the asset functions.\n\n    Args:\n        github_token (str): The GitHub API token.\n        openai_api_key (str): The OpenAI API key.\n        db_connection_string (str): The database connection string.\n        whitelisted_extensions (list[str]): The list of whitelisted file extensions.\n            These extensions will be used to filter the files fetched from the GitHub API.\n        blacklisted_files (list[str]): The list of blacklisted file names.\n            These files will be ignored when fetching data from the GitHub API.\n        embedding_disk_path (str): The path to the disk where the embeddings are stored.\n         The embeddings are temporarily stored on disk before being persisted to the database.\n        embedding_model (str): The OpenAI model used for text embedding.\n        max_embedding_input_length (int): The maximum length of the input text for embedding.\n    \"\"\"\n\n    github_token: str\n    openai_api_key: str\n    db_connection_string: str\n    whitelisted_extensions: list[str]\n    blacklisted_files: list[str]\n    embedding_disk_path: str = \"embeddings\"\n    embedding_model: str = \"text-embedding-3-large\"\n    max_embedding_input_length: int = 8000\n\n    @classmethod\n    def from_env(cls) -&gt; \"AppConfigResource\":\n        \"\"\"\n        Create an instance of the AppConfigResource class from the environment variables.\n\n        Returns:\n            AppConfigResource: The application configuration resource.\n        \"\"\"\n        openai_key = EnvVar(\"OPENAI_EMBEDDING_API_KEY\")\n        github_key = EnvVar(\"GITHUB_API_TOKEN\")\n        conn_str = EnvVar(\"DATABASE_URL\")\n        whitelist = env_var_or_default(\"WHITELISTED_EXTENSIONS\", \"'[\\\"py\\\"]'\", log)\n        blacklist = env_var_or_default(\"BLACKLISTED_FILES\", \"[]\", log)\n\n        if os.getenv(\"DATABASE_URL\") is None:\n            logging.warning(\"DATABASE_URL is not set. Using dev configuration.\")\n            conn_str = \"postgresql://postgres:postgres@localhost:5432/chatGITpt\"\n\n        whitelist = whitelist_str_as_list(whitelist)\n        blacklist = whitelist_str_as_list(blacklist)\n\n        return cls(\n            github_token=github_key,\n            openai_api_key=openai_key,\n            db_connection_string=conn_str,\n            whitelisted_extensions=whitelist,\n            blacklisted_files=blacklist,\n        )\n\n    def get_app_config(self) -&gt; AppConfig:\n        \"\"\"\n        Get an instance of the AppConfig class from the resource configuration. This is\n        because the actual code lives apart from the Dagster framework, and the AppConfig\n        class is used in application code.\n\n        Returns:\n            AppConfig: The application configuration.\n        \"\"\"\n        return AppConfig(\n            github_token=self.github_token,\n            openai_api_key=self.openai_api_key,\n            db_connection_string=self.db_connection_string,\n            embedding_disk_path=self.embedding_disk_path,\n            embedding_model=self.embedding_model,\n            whitelisted_extensions=self.whitelisted_extensions,\n            blacklisted_files=self.blacklisted_files,\n            max_embedding_input_length=self.max_embedding_input_length,\n        )\n</code></pre>"},{"location":"pipelines/reference/resources/#pipelines.orchestration.resources.AppConfigResource.from_env","title":"<code>from_env()</code>  <code>classmethod</code>","text":"<p>Create an instance of the AppConfigResource class from the environment variables.</p> <p>Returns:</p> Name Type Description <code>AppConfigResource</code> <code>AppConfigResource</code> <p>The application configuration resource.</p> Source code in <code>pipelines/pipelines/orchestration/resources.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; \"AppConfigResource\":\n    \"\"\"\n    Create an instance of the AppConfigResource class from the environment variables.\n\n    Returns:\n        AppConfigResource: The application configuration resource.\n    \"\"\"\n    openai_key = EnvVar(\"OPENAI_EMBEDDING_API_KEY\")\n    github_key = EnvVar(\"GITHUB_API_TOKEN\")\n    conn_str = EnvVar(\"DATABASE_URL\")\n    whitelist = env_var_or_default(\"WHITELISTED_EXTENSIONS\", \"'[\\\"py\\\"]'\", log)\n    blacklist = env_var_or_default(\"BLACKLISTED_FILES\", \"[]\", log)\n\n    if os.getenv(\"DATABASE_URL\") is None:\n        logging.warning(\"DATABASE_URL is not set. Using dev configuration.\")\n        conn_str = \"postgresql://postgres:postgres@localhost:5432/chatGITpt\"\n\n    whitelist = whitelist_str_as_list(whitelist)\n    blacklist = whitelist_str_as_list(blacklist)\n\n    return cls(\n        github_token=github_key,\n        openai_api_key=openai_key,\n        db_connection_string=conn_str,\n        whitelisted_extensions=whitelist,\n        blacklisted_files=blacklist,\n    )\n</code></pre>"},{"location":"pipelines/reference/resources/#pipelines.orchestration.resources.AppConfigResource.get_app_config","title":"<code>get_app_config()</code>","text":"<p>Get an instance of the AppConfig class from the resource configuration. This is because the actual code lives apart from the Dagster framework, and the AppConfig class is used in application code.</p> <p>Returns:</p> Name Type Description <code>AppConfig</code> <code>AppConfig</code> <p>The application configuration.</p> Source code in <code>pipelines/pipelines/orchestration/resources.py</code> <pre><code>def get_app_config(self) -&gt; AppConfig:\n    \"\"\"\n    Get an instance of the AppConfig class from the resource configuration. This is\n    because the actual code lives apart from the Dagster framework, and the AppConfig\n    class is used in application code.\n\n    Returns:\n        AppConfig: The application configuration.\n    \"\"\"\n    return AppConfig(\n        github_token=self.github_token,\n        openai_api_key=self.openai_api_key,\n        db_connection_string=self.db_connection_string,\n        embedding_disk_path=self.embedding_disk_path,\n        embedding_model=self.embedding_model,\n        whitelisted_extensions=self.whitelisted_extensions,\n        blacklisted_files=self.blacklisted_files,\n        max_embedding_input_length=self.max_embedding_input_length,\n    )\n</code></pre>"}]}