import asyncio
from datetime import date
from shared.database import EmbeddedDocumentModel, GithubFileModel, TokenSpendModel
from openai import AsyncOpenAI
from dataclasses import dataclass
from typing import Optional, Protocol, Self
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import Date, select, cast
from pydantic import BaseModel, model_validator
import aiohttp
import uuid
from backend.errors import InputError, MaximumSpendError, map_errors
from html import escape


@dataclass(frozen=True)
class RetrievalAugmentedGeneration:
    """
    Class that performs retrieval-augmented generation given an input query and k value.

    Args:
        embedding_service: The embedding service used for text embedding.
        retrieval_service: The retrieval service used for context retrieval.
        generation_service: The generation service used for text generation.
        max_spend: The maximum spend limit for the retrieval-augmented generation.
        date (datetime.date): The date of the retrieval-augmented generation.
    """

    embedding_service: "EmbeddingService"
    retrieval_service: "RetrievalService"
    generation_service: "GenerationService"
    max_spend: float
    date: date

    async def retrieval_augmented_generation(
        self, input: "InputQuery", k: int
    ) -> "RAGResponse":
        """
        Performs retrieval-augmented generation given an input query and k value.
        The method enforces the spend limit, validates the session ID, retrieves the top k contexts,

        Args:
            input (InputQuery): The input query. Contains the user's question, and optionally,
                the previous context and session ID.
            k (int): The number of contexts to retrieve.

        Returns:
            (RAGResponse): The response generated by the model and the session ID.

        Raises:
            (MaximumSpendError): If the current spend is greater than or equal to the maximum spend limit.
        """
        await self._enforce_spend_limit()
        session_id = await self._validate_and_assign_session_id(input)

        retrieved, tokens_spent = await self._retrieve_top_k(input, k)
        store_task = self.retrieval_service.store_token_spent(
            session_id,
            tokens_spent,
            self.embedding_service.get_embed_model_name(),
        )
        generation_task = self.generation_service.augmented_generation(input, retrieved)
        _, response_and_tokens = await asyncio.gather(store_task, generation_task)
        response, tokens = response_and_tokens
        await self.retrieval_service.store_token_spent(
            session_id, tokens, self.generation_service.get_chat_model_name()
        )

        return RAGResponse(response=response, session_id=session_id)

    async def remaining_spend(self) -> "RemainingSpend":
        """

        Calculates the remaining spend based on the current spend and the maximum spend limit.
        The spend is capped at 0.

        Returns:
            (RemainingSpend): The remaining spend.
        """
        current_spend = await self.retrieval_service.get_current_spend(self.date)
        remaining = round(self.max_spend - current_spend, 2)
        remaining = max(remaining, 0)
        return RemainingSpend(remaining)

    async def _enforce_spend_limit(self) -> None:
        """

        Enforces the spend limit by checking the current spend against the maximum spend limit.
        If the current spend is greater than or equal to the maximum spend limit, raises a `MaximumSpendError`.

        Raises:
            (MaximumSpendError): If the current spend is greater than or equal to the maximum spend limit.

        """
        current_spend = await self.retrieval_service.get_current_spend(self.date)
        if current_spend >= self.max_spend:
            raise MaximumSpendError()

    async def _validate_and_assign_session_id(self, input: "InputQuery") -> str:
        """

        Validates the session ID if provided, otherwise generates a new session ID.
        This method ensures that the session ID is valid and exists in the database.
        This is necessary to track the token spend for the given session.

        Args:
            input (InputQuery): The input query. Contains the user's question, and optionally,
                the previous context and session ID.

        Returns:
            (str): The session ID.
        """
        if input.session_id:
            await self.retrieval_service.validate_session_id(input.session_id)
            session_id = input.session_id
        else:
            session_id = str(uuid.uuid4())
        return session_id

    async def _retrieve_top_k(
        self, input: "InputQuery", k: int
    ) -> tuple[list["RetrievedContext"], int]:
        """
            embeds the input query and retrieves the top k contexts based on the embedded query.


        Returns:
            (tuple[list[RetrievedContext], int]): A tuple containing the list of retrieved contexts
                and the number of tokens spent. see `RetrievedContext` for more information.
        """
        result = await self.embedding_service.embed(input.query)
        tokens_spent = result.token_count
        return (await self.retrieval_service.retrieve_top_k(result, k), tokens_spent)


class RetrievalService(Protocol):
    """
    Interface for the retrieval service. Responsible for retrieving the top k
    contexts based on the embedded query. The query is embedded already by the
    `EmbeddingService`.

    """

    async def retrieve_top_k(
        self, embedded_query: "EmbeddedResponse", k: int
    ) -> list["RetrievedContext"]:
        """

        Retrieves the top k contexts based on the embedded query.
        The contexts are the k most relevant documents to the embedded query.

        Args:
            embedded_query (EmbeddedResponse): The embedded query.
            k (int): The number of contexts to retrieve.

        Returns:
            (list[RetrievedContext]): The list of retrieved contexts.
        """
        ...

    async def store_token_spent(
        self, session_id: str, token_count: int, model_name: str
    ):
        """

        Stores the token count spent for a given session ID and model name.
        This is necessary to track the token spend for the given session. This
        is used further down the line to calculate the current spend and enforce
        the spend limit.

        Args:
            session_id (str): The session ID.
            token_count (int): The number of tokens spent.
            model_name (str): The name of the model used for token count. Different
                models have different token costs. Embedding models are
                typically cheaper than generation models.
        """

        ...

    async def validate_session_id(self, session_id: str) -> None:
        """

        Validates the session ID if provided. This method ensures that the session ID is valid
        and exists in the database. This is necessary to track the token spend for the given session.

        Args:
            session_id (str): The session ID.
        """
        ...

    async def get_current_spend(self, date: date) -> float:
        """

        Retrieves the current spend for a given date. The date is
        mostly given as a parameter to facilitate testing, it keeps
        this method more pure than it is if it had to instantiate the date.

        Args:
            date (date): The date for which to retrieve the current spend.


        Returns:
            (float): The current spend for the given date.
        """
        ...


@dataclass(frozen=True, slots=True)
class RemainingSpend:
    """
    Class that represents the remaining spend based on the current spend and the maximum spend limit.
    This is the object that will later be deserialized to JSON and returned to the user.

    Args:
        remaining_spend (float): The remaining spend based on the current spend and the maximum spend limit.
    """

    remaining_spend: float


class GenerationService(Protocol):
    """
    The interface for the generation service. Responsible for generating the response
    given the input query and the retrieved contexts.
    """

    async def augmented_generation(
        self, query: "InputQuery", context: list["RetrievedContext"]
    ) -> tuple[str, int]:
        """Answers the user's query with the retrieved context.
        Requires that you already have the context retrieved from the `RetrievalService`.

        Args:
            query (InputQuery): The input query. Contains the user's question, and optionally,
                the previous context and session ID.
            context (list[RetrievedContext]): The list of retrieved contexts. Each one contains
                the distance, file name, repository name, path in the repository, extension, and URL.

        Returns:
            (tuple[str, int]): The response generated by the model and the number of tokens spent.
        """
        ...

    def get_chat_model_name(self) -> str:
        """Returns the name of the chat model used for generation. Needed for tracking the token spend
           per model. Each model has a different token cost, e.g., the embedding model is typically cheaper
           than the generation model.


        Returns:
            (str): The name of the chat model used for generation.
        """
        ...


class EmbeddingService(Protocol):
    """
    The interface for the embedding service. Responsible for embedding the input query.
    The embedded query is then used to retrieve the top k contexts based on the
    embedded query by the `RetrievalService`.
    """

    async def embed(self, text: str) -> "EmbeddedResponse":
        """
        Embeds the input text. Turns the input text into a list of floats that represent the text.
        The embedded response also contains the number of tokens spent on the embedding.

        Args:
            text (str): The text to embed.

        Returns:
            (EmbeddedResponse): The embedded response. Contains the embedding and the number of tokens spent.
        """
        ...

    def get_embed_model_name(self) -> str:
        """
        Returns the name of the embedding model used for embedding. Needed for tracking the token spend
        per model. Each model has a different token cost, e.g., the embedding model is typically cheaper
        than the generation model.

        Returns:
            (str): The name of the embedding model used for embedding.
        """
        ...


class PreviousQAs(BaseModel):
    """
    A Pydantic model for the previous question-answer pairs. This is used to provide the model with
    additional context from previous interactions. It is a pydantic model because it is client
    facing and is used to validate the input query. This model is used in the `InputQuery` model.

    Args:
        question (str): A previously asked question asked by the user.
        answer (str): A previously received answer provided by the assistant.
    """

    question: str
    answer: str


class InputQuery(BaseModel):
    """
    A Pydantic model for the input query. This class is client facing and is used to validate the input query.
    The first question in the conversation will not have any previous context and will not have a session ID.
    The subsequent questions will have a session ID and previous context. The previous context is a list of
    question-answer pairs from the previous interactions. The session ID is used to track the token spend for
    the given session.

    If the session ID is provided, the previous context must also be provided otherwise a `ValueError` is raised.

    Raises:
        (ValueError): If the session ID is provided but the previous context is not provided or vice versa.
    """

    query: str
    previous_context: Optional[list[PreviousQAs]] = None
    session_id: Optional[str] = None

    def previous_context_to_dict(self) -> list[dict[str, str]]:
        """
        Transforms the previous context from a list of `PreviousQAs` to a list of dictionaries.
        This is the exact format that OpenAI's SDK expects for its completion API.

        Returns:
            list[dict[str, str]]: The previous context as a list of dictionaries.
        """
        res = []
        if self.previous_context:
            for qa in self.previous_context:
                res.append({"role": "user", "content": qa.question})
                res.append({"role": "assistant", "content": qa.answer})
        return res

    @model_validator(mode="after")
    def _validate_previous_context(self) -> Self:
        """
        Validates the input query. Ensures that the session ID is provided if the previous context is provided
        and vice versa.

        Raises:
            (ValueError): If the session ID is provided but the previous context is not provided or vice versa.

        """
        if self.session_id and not self.previous_context:
            raise ValueError(
                "Previous context must be provided if session_id is present."
            )
        elif not self.session_id and self.previous_context:
            raise ValueError(
                "Session ID must be provided if previous context is present."
            )
        return self


@dataclass(frozen=True, slots=True)
class RAGResponse:
    """
    A class that represents the response generated by the model and the session ID.
    This data class is client facing and will be deserialized to JSON and returned to the user.
    It contains a session_id because the client needs to provide the session ID in the subsequent
    questions for tracking the token spend for the given session.
    """

    response: str
    session_id: str


@dataclass(frozen=True, slots=True)
class RetrievedContext:
    """
    This is a domain class and is used to represent the context retrieved from the database. Its
    main purpose is to provide a structured representation of the context that is retrieved from the
    database. This simplifies testing, all of the logic relies on domain objects rather than database
    specific types.

    The class also provides helpers to enrich the file content with additional information such as
    the file name, repository name, path in the repository, and file extension. This is useful for
    generating the response to the user.

    Args:
        distance (float): The distance between the embedded query and the retrieved context.
        file_name (str): The name of the file.
        repository_name (str): The name of the repository.
        path_in_repo (str): The path of the file in the repository.
        extension (str): The file extension.
        url (str): The URL to the file content.
    """

    distance: float
    file_name: str
    repository_name: str
    path_in_repo: str
    extension: str
    url: str

    @classmethod
    def from_document(
        cls, score: float, document: GithubFileModel
    ) -> "RetrievedContext":
        """Creates a `RetrievedContext` object from the document retrieved from the database.


        Args:
            score (float): The distance between the embedded query and the retrieved context.
            document (GithubFileModel): The document retrieved from the database.

        Returns:
            (RetrievedContext): The retrieved context object.
        """
        return cls(
            distance=score,
            file_name=document.name,
            repository_name=document.repository_name,
            path_in_repo=document.path_in_repo,
            extension=document.file_extension,
            url=document.content_url,
        )

    async def to_context(self) -> str:
        """Retrieves the file content from the URL and enriches it with additional information such as
        the file name, repository name, path in the repository, and file extension. This may
        improve generation quality by providing additional context to the model. The method is asynchronous
        because it fetches the file content from the URL. The data isn't persisted as such, it is fetched
        on-demand when needed.

        Returns:
            (str): The file content enriched with additional information.
        """
        async with aiohttp.ClientSession() as session:
            async with session.get(self.url) as response:
                with map_errors():
                    file_content = await response.text()
        return self._enrich_file_content(file_content)

    def _enrich_file_content(self, file_content: str) -> str:
        """Enriches the file content with additional information such as the file name, repository name,
        path in the repository, and file extension. This may improve generation quality by providing
        additional context to the model.

        Args:
            file_content (str): The file content.

        Returns:
            (str): The file content enriched with additional information.
        """
        file_name = f"\nThe file name is {self.file_name}.\n"
        file_place_in_project = f"The file is located at {self.path_in_repo}.\n"
        file_extension = f"The file extension is {self.extension}.\n"
        return file_content + file_name + file_place_in_project + file_extension


@dataclass(frozen=True, slots=True)
class EmbeddedResponse:
    """
    A class that represents the embedded response. This class is used to represent the response
    generated by the embedding service. The embedded response contains the embedding and the number
    of tokens spent on the embedding. This data class is used to pass the embedded response between
    the `EmbeddingService` and the `RetrievalService`.

    Args:
        embedding (list[float]): The embedded query.
        token_count (int): The number of tokens spent on the embedding.
    """

    embedding: list[float]
    token_count: int


@dataclass(frozen=True)
class OpenAIEmbeddingService(EmbeddingService):
    """
    A class that performs text embedding using OpenAI's API. The class is responsible for embedding
    the input text. The embedded text is then used to retrieve the top k contexts based on the embedded
    query by the `RetrievalService`. The class is a wrapper around OpenAI's SDK and provides a more
    testable and maintainable interface. It implements the `EmbeddingService` interface.

    Args:
        client (AsyncOpenAI): The OpenAI client used for text embedding.
            Relies on the `OPENAI_EMBEDDING_API_KEY` environment variable.
        embedding_model (str): The name of the embedding model used for text embedding.
            In practice this is `text-embedding-3-large` but it is configurable through environment variables.
            It can directly be set with the `EMBEDDING_MODEL`.
    """

    client: AsyncOpenAI
    embedding_model: str

    async def embed(self, text: str) -> EmbeddedResponse:
        """
        Embeds the input text. Turns the input text into a list of floats that represent the text.
        The embedded response also contains the number of tokens spent on the embedding.

        Args:
            text (str): The text to embed.

        Returns:
            EmbeddedResponse: The embedded response. Contains the embedding and the number of tokens spent.
        """
        with map_errors():
            response = await self.client.embeddings.create(
                input=[text], model=self.embedding_model
            )
        return EmbeddedResponse(
            embedding=response.data[0].embedding,
            token_count=response.usage.total_tokens,
        )

    def get_embed_model_name(self) -> str:
        """
        Wrapper around the `embedding_model` attribute. Necessary to conform to the `EmbeddingService` interface.

        Returns:
            str: The name of the embedding model used for embedding.
        """
        return self.embedding_model


@dataclass(frozen=True, slots=True)
class OpenAIGenerationService(GenerationService):
    """
    A class that performs text generation using OpenAI's API. The class is responsible for generating
    the response given the input query and the retrieved contexts. The class is a wrapper around OpenAI's
    SDK and provides a more testable and maintainable interface. It implements the `GenerationService` interface.

    Args:
        client (AsyncOpenAI): The OpenAI client used for text generation.
            Relies on the `OPENAI_EMBEDDING_API_KEY` environment variable.
        chat_model (str): The name of the chat model used for text generation.
            Can be set with the `CHAT_MODEL` environment variable.
        system_prompt (str): The system prompt used for text generation.
            Can be set with the `SYSTEM_PROMPT` environment variable.
    """

    client: AsyncOpenAI
    chat_model: str
    system_prompt: str

    async def augmented_generation(
        self, query: InputQuery, context: list[RetrievedContext]
    ) -> tuple[str, int]:
        """
        Answers the user's query with the retrieved context. Requires that you already have the context
        retrieved from the `RetrievalService`.

        Args:
            query (InputQuery): The input query. Contains the user's question, and optionally,
                the previous context and session ID.
            context (list[RetrievedContext]): The list of retrieved contexts. Each one contains
                the distance, file name, repository name, path in the repository, extension, and URL.

        Returns:
            (tuple[str, int]): The response generated by the model and the number of tokens spent.
        """
        ctx = await asyncio.gather(*[c.to_context() for c in context])
        model_input = self._create_model_input(query, ctx)
        with map_errors():
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=model_input,  # type: ignore
            )
        result = response.choices[0].message.content or ""
        tokens = response.usage.total_tokens if response.usage else 0
        return add_sources(result, context), tokens

    def get_chat_model_name(self) -> str:
        """Returns the name of the chat model used for generation. Needed for tracking the token spend
            per model. Each model has a different token cost, e.g., the embedding model is typically cheaper
            than the generation model.

        Returns:
            (str): The name of the chat model used for generation.
        """
        return self.chat_model

    def _create_model_input(
        self, input: InputQuery, ctx: list[str]
    ) -> list[dict[str, str]]:
        """Creates the model input for the generation API. The model input is a list of dictionaries."""
        query_with_ctx = f"""The user's question is {input.query}.
        You have access to additional context in a list of code files: {ctx}"""

        previous_qas = input.previous_context_to_dict()

        model_input = [{"role": "system", "content": self.system_prompt}]
        model_input.extend(previous_qas)
        model_input.append({"role": "user", "content": query_with_ctx})
        return model_input


def add_sources(answer: str, retrieved: list[RetrievedContext]) -> str:
    """
    Adds the sources to the answer. The sources are the list of retrieved contexts.
    The sources are added as an HTML section at the end of the answer.
    Args:
        answer (str): The answer generated by the model.
        retrieved (list[RetrievedContext]): The list of retrieved contexts.
    Returns:
        (str): The answer with the sources added as an HTML section at the end.
    """
    sources = [
        f"<li>{escape(r.file_name)} in {escape(r.repository_name)}, <a href={escape(r.url)}>source.</a></li>"
        for r in retrieved
    ]
    return f"""{answer}\n <section id="sources">To answer this question, I used the following sources:
        <ul>{''.join(sources)}</ul></section>"""


@dataclass(frozen=True, slots=True)
class SQLRetrievalService(RetrievalService):
    """
    A class that performs context retrieval using SQL. The class is responsible for retrieving the top k
    contexts based on the embedded query. The query is embedded already by the `EmbeddingService`. The class
    is a wrapper around SQLAlchemy and provides a more testable and maintainable interface. It implements
    the `RetrievalService` interface.

    Aside from this it is also responsible for storing the token spend and validating the session ID.

    Raises:
        (InputError): If a Session ID is provided, it must already exist.
    """

    async_session: AsyncSession

    async def retrieve_top_k(
        self, embedded_query: EmbeddedResponse, k: int
    ) -> list[RetrievedContext]:
        """
        Retrieves the top k contexts based on the embedded query. The contexts are the k most relevant
        documents to the embedded query. The distance between the embedded query and the retrieved context
        is given by the cosine distance.

        In short, the lower the distance, the more similar the context is to the query.
        This is why we order by distance and limit the number of contexts to k.

        Args:
            embedded_query (EmbeddedResponse): The embedded query. This is done by the `EmbeddingService`.
            k (int): The number of contexts to retrieve.
                Can be set with the `TOP_K` environment variable. The default value is 5 but it is configurable.
                Increasing it will also increase the token spend.

        Returns:
            (list[RetrievedContext]): The list of retrieved contexts.
        """

        stmt = (
            select(
                EmbeddedDocumentModel.embedding.cosine_distance(
                    embedded_query.embedding
                ).label("distance"),
                EmbeddedDocumentModel,
            )
            .join(
                GithubFileModel, EmbeddedDocumentModel.document_id == GithubFileModel.id
            )
            .order_by("distance")
            .limit(k)
        )

        async with self.async_session.begin():
            with map_errors():
                result = await self.async_session.execute(stmt)
                documents = result.all()

        return [
            RetrievedContext.from_document(doc[0], doc[1].document) for doc in documents
        ]

    async def store_token_spent(
        self,
        session_id: str,
        token_count: int,
        model_name: str,
    ) -> None:
        """
        In multiple steps of the RAG pipeline, we need to store the token count spent for a given session ID
        and model name. This is necessary to track the token spend for the given session. This is used further
        down the line to calculate the current spend and enforce the spend limit.

        The moments where we need to store the token count spent are:
        - After the embedding step.
        - After the generation step.

        The moments this is done may also increase as the RAG is further developed.


        Args:
            session_id (str): The session ID. Corresponds to a single conversation.
            token_count (int): The number of tokens spent.
            model_name (str): The name of the model used. Different models have different token costs.
        """
        token_spend = TokenSpendModel(
            session_id=session_id,
            token_count=token_count,
            model=model_name,
        )
        async with self.async_session.begin():
            with map_errors():
                self.async_session.add(token_spend)
                await self.async_session.commit()

    async def get_current_spend(self, date: date) -> float:
        """
        Retrieves the current spend for a given date. The date is mostly given as a parameter to facilitate
        testing, it keeps this method more pure than it is if it had to instantiate the date.

        The current spend is given as follows:
        - The token count spent for the given date is retrieved.
        - The token count spent is multiplied by 0.00001 to get the spend in dollars.
        - The sum of all the spends is returned.

        0.00001 is taken as a constant, it's the approximate cost, taking into account both model costs. It's a
        conservative estimate, the actual cost may be lower.


        Args:
            date (datetime.date): The date for which to retrieve the current spend.

        Returns:
            (float): The current spend for the given date.
        """
        stmt = select(TokenSpendModel).where(
            cast(TokenSpendModel.timestamp, Date) == date
        )
        async with self.async_session.begin():
            with map_errors():
                result = await self.async_session.execute(stmt)
            return sum([r.token_count for r in result.scalars()]) * 0.00001

    async def validate_session_id(self, session_id: str) -> None:
        """
        Validates the session ID if provided. This method ensures that the session ID is valid
        and exists in the database. This is necessary to track the token spend for the given session.

        Args:
            session_id (str): The unique id of the conversation.

        Raises:
            (InputError): If a Session ID is provided, it must already exist.
        """
        stmt = select(TokenSpendModel).where(TokenSpendModel.session_id == session_id)
        async with self.async_session.begin():
            with map_errors():
                result = await self.async_session.execute(stmt)
            if not result.scalar():
                raise InputError("If a Session ID is provided, it must already exist.")
